{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Project Layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage with project summary Broker_Performance.md # Predicting broker performance LCDA_Part2.md # Clustering & PCA LCDA_Part3.md # Building models Summary Objective: To evaluate and predict brokers performance for an insurance company so that the company can better manage future plans, and objectives. This can be done by partitioning brokers into segments by applying a clustering methodologies based on the most recent years of information (ending w/ 2018). Then we need to predict if the gross written premium will either increase or decrease for 2018 and 2019. Tasks for this project are; 1. Broker Segmentation 2. Build a robust machine learning model to forecast whether \u201cGrowth Written Premium\u201d will increase or decrease for FY 2019 Data Preprocessing We begin by loading all necessary libraries and setting a random seed generator library(dplyr) library(reshape2) library(caret) library(rpart) library(cluster) library(rgl) # 3d library(fpc) library(dummies) library(ggplot2) library(ROCR) library(randomForest) library(GGally) set.seed(12345) Loading data The data has 30 columns and a total of 189 rows. The .csv file was read using the \u201cread.table\u201d function and saved as .rda file. load(\"alchemyBrokerData.rda\") We can find the total number of missing values by using the sum(is.na()) function; we can see the data has 1323 NAs. # Calculating NAs sum(is.na(brokerData)) ## [1] 1323 # Checking the number of NULLs in submissions from 2015 to 2018 (max NAs in 2015) sum(is.na(brokerData$Submissions_2015)) sum(is.na(brokerData$Submissions_2016)) sum(is.na(brokerData$Submissions_2017)) sum(is.na(brokerData$Submissions_2018)) ## [1] 60 ## [1] 45 ## [1] 30 ## [1] 8 Handling missing values Some of the techniques for imputing missing values are; deleting all rows that contained NA\u2019s, imputing missing data with the mean or mode, and/or replacing missing values with zero's. However, none of these are appropriate for this case which brings us to the next best option: using either a classification or regression method for predicting and imputing the missing information to handle the missing values. # Creating a data frame with all the required variables myBDF <- brokerData %>% dplyr::select(Submissions_2014:AvgTIV_2018) # We will use the entire dataset for prediction of NAs and then get rid of the excess columns (2013 & 2014) # 2015 sub2015Rpart <- rpart(myBDF$Submissions_2015 ~ ., data=myBDF) sub2015pred <- round(predict(sub2015Rpart, newdata=myBDF, type=\"vector\")) myBDF$Submissions_2015[is.na(myBDF$Submissions_2015)] <- sub2015pred[is.na(myBDF$Submissions_2015)] # 2016 sub2016Rpart <- rpart(myBDF$Submissions_2016 ~ ., data=myBDF) sub2016pred <- round(predict(sub2016Rpart, newdata=myBDF, type=\"vector\")) myBDF$Submissions_2016[is.na(myBDF$Submissions_2016)] <- sub2016pred[is.na(myBDF$Submissions_2016)] # 2017 sub2017Rpart <- rpart(myBDF$Submissions_2017 ~ ., data=myBDF) sub2017pred <- round(predict(sub2017Rpart, newdata=myBDF, type=\"vector\")) myBDF$Submissions_2017[is.na(myBDF$Submissions_2017)] <- sub2017pred[is.na(myBDF$Submissions_2017)] # 2018 sub2018Rpart <- rpart(myBDF$Submissions_2018 ~ ., data=myBDF) sub2018pred <- round(predict(sub2018Rpart, newdata=myBDF, type=\"vector\")) myBDF$Submissions_2018[is.na(myBDF$Submissions_2018)] <- sub2018pred[is.na(myBDF$Submissions_2018)] # 2015 quote2015RPART <- rpart(formula = QuoteCount_2015 ~ ., data=myBDF) quote2015pred <- round(predict(quote2015RPART, newdata=myBDF)) myBDF$QuoteCount_2015[is.na(myBDF$QuoteCount_2015)] <- quote2015pred[is.na(myBDF$QuoteCount_2015)] # 2016 quote2016RPART <- rpart(formula = QuoteCount_2016 ~ ., data=myBDF) quote2016pred <- round(predict(quote2016RPART, newdata=myBDF)) myBDF$QuoteCount_2016[is.na(myBDF$QuoteCount_2016)] <- quote2016pred[is.na(myBDF$QuoteCount_2016)] # 2017 quote2017RPART <- rpart(formula = QuoteCount_2017 ~ ., data=myBDF) quote2017pred <- round(predict(quote2017RPART, newdata=myBDF)) myBDF$QuoteCount_2017[is.na(myBDF$QuoteCount_2017)] <- quote2017pred[is.na(myBDF$QuoteCount_2017)] # 2018 quote2018RPART <- rpart(formula = QuoteCount_2018 ~ ., data=myBDF) quote2018pred <- round(predict(quote2018RPART, newdata=myBDF)) myBDF$QuoteCount_2018[is.na(myBDF$QuoteCount_2018)] <- quote2018pred[is.na(myBDF$QuoteCount_2018)] # Policy count # 2015 policy2015RPART <- rpart(formula = PolicyCount_2015 ~ ., data=myBDF) pol2015pred <- round(predict(policy2015RPART, newdata=myBDF)) myBDF$PolicyCount_2015[is.na(myBDF$PolicyCount_2015)] <- pol2015pred[is.na(myBDF$PolicyCount_2015)] # 2016 policy2016RPART <- rpart(formula = PolicyCount_2016 ~ ., data=myBDF) pol2016pred <- round(predict(policy2016RPART, newdata=myBDF)) myBDF$PolicyCount_2016[is.na(myBDF$PolicyCount_2016)] <- pol2016pred[is.na(myBDF$PolicyCount_2016)] # 2017 policy2017RPART <- rpart(formula = PolicyCount_2017 ~ ., data=myBDF) pol2017pred <- round(predict(policy2017RPART, newdata=myBDF)) myBDF$PolicyCount_2017[is.na(myBDF$PolicyCount_2017)] <- pol2017pred[is.na(myBDF$PolicyCount_2017)] # 2018 policy2018RPART <- rpart(formula = PolicyCount_2018 ~ ., data=myBDF) pol2018pred <- round(predict(policy2018RPART, newdata=myBDF)) myBDF$PolicyCount_2018[is.na(myBDF$PolicyCount_2018)] <- pol2018pred[is.na(myBDF$PolicyCount_2018)] # AvgQuote # 2015 avgq2015RPART <- rpart(formula = AvgQuote_2015 ~ ., data=myBDF) avgq2015pred <- (predict(avgq2015RPART, newdata=myBDF)) myBDF$AvgQuote_2015[is.na(myBDF$AvgQuote_2015)] <- avgq2015pred[is.na(myBDF$AvgQuote_2015)] # 2016 avgq2016RPART <- rpart(formula = AvgQuote_2016 ~ ., data=myBDF) avgq2016pred <- (predict(avgq2016RPART, newdata=myBDF)) myBDF$AvgQuote_2016[is.na(myBDF$AvgQuote_2016)] <- avgq2016pred[is.na(myBDF$AvgQuote_2016)] # 2017 avgq2017RPART <- rpart(formula = AvgQuote_2017 ~ ., data=myBDF) avgq2017pred <- (predict(avgq2017RPART, newdata=myBDF)) myBDF$AvgQuote_2017[is.na(myBDF$AvgQuote_2017)] <- avgq2017pred[is.na(myBDF$AvgQuote_2017)] # 2018 avgq2018RPART <- rpart(formula = AvgQuote_2018 ~ ., data=myBDF) avgq2018pred <- (predict(avgq2018RPART, newdata=myBDF)) myBDF$AvgQuote_2018[is.na(myBDF$AvgQuote_2018)] <- avgq2018pred[is.na(myBDF$AvgQuote_2018)] # TIV # 2015 TIV2015RPART <- rpart(formula = AvgTIV_2015 ~ ., data=myBDF) TIV2015pred <- round(predict(TIV2015RPART, newdata=myBDF)) myBDF$AvgTIV_2015[is.na(myBDF$AvgTIV_2015)] <- TIV2015pred[is.na(myBDF$AvgTIV_2015)] # 2016 TIV2016RPART <- rpart(formula = AvgTIV_2016 ~ ., data=myBDF) TIV2016pred <- round(predict(TIV2016RPART, newdata=myBDF)) myBDF$AvgTIV_2016[is.na(myBDF$AvgTIV_2016)] <- TIV2016pred[is.na(myBDF$AvgTIV_2016)] # 2017 TIV2017RPART <- rpart(formula = AvgTIV_2017 ~ ., data=myBDF) TIV2017pred <- round(predict(TIV2017RPART, newdata=myBDF)) myBDF$AvgTIV_2017[is.na(myBDF$AvgTIV_2017)] <- TIV2017pred[is.na(myBDF$AvgTIV_2017)] # 2018 TIV2018RPART <- rpart(formula = AvgTIV_2018 ~ ., data=myBDF) TIV2018pred <- round(predict(TIV2018RPART, newdata=myBDF)) myBDF$AvgTIV_2018[is.na(myBDF$AvgTIV_2018)] <- TIV2018pred[is.na(myBDF$AvgTIV_2018)] # GWP # 2015 GWP2015RPART <- rpart(formula = GWP_2015 ~ ., data=myBDF) GWP2015pred <- (predict(GWP2015RPART, newdata=myBDF)) myBDF$GWP_2015[is.na(myBDF$GWP_2015)] <- GWP2015pred[is.na(myBDF$GWP_2015)] # 2016 GWP2016RPART <- rpart(formula = GWP_2016 ~ ., data=myBDF) GWP2016pred <- (predict(GWP2016RPART, newdata=myBDF)) myBDF$GWP_2016[is.na(myBDF$GWP_2016)] <- GWP2016pred[is.na(myBDF$GWP_2016)] # 2017 GWP2017RPART <- rpart(formula = GWP_2017 ~ ., data=myBDF) GWP2017pred <- (predict(GWP2017RPART, newdata=myBDF)) myBDF$GWP_2017[is.na(myBDF$GWP_2017)] <- GWP2017pred[is.na(myBDF$GWP_2017)] # 2018 GWP2018RPART <- rpart(formula = GWP_2018 ~ ., data=myBDF) GWP2018pred <- (predict(GWP2018RPART, newdata=myBDF)) myBDF$GWP_2018[is.na(myBDF$GWP_2018)] <- GWP2018pred[is.na(myBDF$GWP_2018)]","title":"Home"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage with project summary Broker_Performance.md # Predicting broker performance LCDA_Part2.md # Clustering & PCA LCDA_Part3.md # Building models","title":"Project Layout"},{"location":"#summary","text":"Objective: To evaluate and predict brokers performance for an insurance company so that the company can better manage future plans, and objectives. This can be done by partitioning brokers into segments by applying a clustering methodologies based on the most recent years of information (ending w/ 2018). Then we need to predict if the gross written premium will either increase or decrease for 2018 and 2019. Tasks for this project are; 1. Broker Segmentation 2. Build a robust machine learning model to forecast whether \u201cGrowth Written Premium\u201d will increase or decrease for FY 2019","title":"Summary"},{"location":"#data-preprocessing","text":"We begin by loading all necessary libraries and setting a random seed generator library(dplyr) library(reshape2) library(caret) library(rpart) library(cluster) library(rgl) # 3d library(fpc) library(dummies) library(ggplot2) library(ROCR) library(randomForest) library(GGally) set.seed(12345)","title":"Data Preprocessing"},{"location":"#loading-data","text":"The data has 30 columns and a total of 189 rows. The .csv file was read using the \u201cread.table\u201d function and saved as .rda file. load(\"alchemyBrokerData.rda\") We can find the total number of missing values by using the sum(is.na()) function; we can see the data has 1323 NAs. # Calculating NAs sum(is.na(brokerData)) ## [1] 1323 # Checking the number of NULLs in submissions from 2015 to 2018 (max NAs in 2015) sum(is.na(brokerData$Submissions_2015)) sum(is.na(brokerData$Submissions_2016)) sum(is.na(brokerData$Submissions_2017)) sum(is.na(brokerData$Submissions_2018)) ## [1] 60 ## [1] 45 ## [1] 30 ## [1] 8","title":"Loading data"},{"location":"#handling-missing-values","text":"Some of the techniques for imputing missing values are; deleting all rows that contained NA\u2019s, imputing missing data with the mean or mode, and/or replacing missing values with zero's. However, none of these are appropriate for this case which brings us to the next best option: using either a classification or regression method for predicting and imputing the missing information to handle the missing values. # Creating a data frame with all the required variables myBDF <- brokerData %>% dplyr::select(Submissions_2014:AvgTIV_2018) # We will use the entire dataset for prediction of NAs and then get rid of the excess columns (2013 & 2014) # 2015 sub2015Rpart <- rpart(myBDF$Submissions_2015 ~ ., data=myBDF) sub2015pred <- round(predict(sub2015Rpart, newdata=myBDF, type=\"vector\")) myBDF$Submissions_2015[is.na(myBDF$Submissions_2015)] <- sub2015pred[is.na(myBDF$Submissions_2015)] # 2016 sub2016Rpart <- rpart(myBDF$Submissions_2016 ~ ., data=myBDF) sub2016pred <- round(predict(sub2016Rpart, newdata=myBDF, type=\"vector\")) myBDF$Submissions_2016[is.na(myBDF$Submissions_2016)] <- sub2016pred[is.na(myBDF$Submissions_2016)] # 2017 sub2017Rpart <- rpart(myBDF$Submissions_2017 ~ ., data=myBDF) sub2017pred <- round(predict(sub2017Rpart, newdata=myBDF, type=\"vector\")) myBDF$Submissions_2017[is.na(myBDF$Submissions_2017)] <- sub2017pred[is.na(myBDF$Submissions_2017)] # 2018 sub2018Rpart <- rpart(myBDF$Submissions_2018 ~ ., data=myBDF) sub2018pred <- round(predict(sub2018Rpart, newdata=myBDF, type=\"vector\")) myBDF$Submissions_2018[is.na(myBDF$Submissions_2018)] <- sub2018pred[is.na(myBDF$Submissions_2018)] # 2015 quote2015RPART <- rpart(formula = QuoteCount_2015 ~ ., data=myBDF) quote2015pred <- round(predict(quote2015RPART, newdata=myBDF)) myBDF$QuoteCount_2015[is.na(myBDF$QuoteCount_2015)] <- quote2015pred[is.na(myBDF$QuoteCount_2015)] # 2016 quote2016RPART <- rpart(formula = QuoteCount_2016 ~ ., data=myBDF) quote2016pred <- round(predict(quote2016RPART, newdata=myBDF)) myBDF$QuoteCount_2016[is.na(myBDF$QuoteCount_2016)] <- quote2016pred[is.na(myBDF$QuoteCount_2016)] # 2017 quote2017RPART <- rpart(formula = QuoteCount_2017 ~ ., data=myBDF) quote2017pred <- round(predict(quote2017RPART, newdata=myBDF)) myBDF$QuoteCount_2017[is.na(myBDF$QuoteCount_2017)] <- quote2017pred[is.na(myBDF$QuoteCount_2017)] # 2018 quote2018RPART <- rpart(formula = QuoteCount_2018 ~ ., data=myBDF) quote2018pred <- round(predict(quote2018RPART, newdata=myBDF)) myBDF$QuoteCount_2018[is.na(myBDF$QuoteCount_2018)] <- quote2018pred[is.na(myBDF$QuoteCount_2018)] # Policy count # 2015 policy2015RPART <- rpart(formula = PolicyCount_2015 ~ ., data=myBDF) pol2015pred <- round(predict(policy2015RPART, newdata=myBDF)) myBDF$PolicyCount_2015[is.na(myBDF$PolicyCount_2015)] <- pol2015pred[is.na(myBDF$PolicyCount_2015)] # 2016 policy2016RPART <- rpart(formula = PolicyCount_2016 ~ ., data=myBDF) pol2016pred <- round(predict(policy2016RPART, newdata=myBDF)) myBDF$PolicyCount_2016[is.na(myBDF$PolicyCount_2016)] <- pol2016pred[is.na(myBDF$PolicyCount_2016)] # 2017 policy2017RPART <- rpart(formula = PolicyCount_2017 ~ ., data=myBDF) pol2017pred <- round(predict(policy2017RPART, newdata=myBDF)) myBDF$PolicyCount_2017[is.na(myBDF$PolicyCount_2017)] <- pol2017pred[is.na(myBDF$PolicyCount_2017)] # 2018 policy2018RPART <- rpart(formula = PolicyCount_2018 ~ ., data=myBDF) pol2018pred <- round(predict(policy2018RPART, newdata=myBDF)) myBDF$PolicyCount_2018[is.na(myBDF$PolicyCount_2018)] <- pol2018pred[is.na(myBDF$PolicyCount_2018)] # AvgQuote # 2015 avgq2015RPART <- rpart(formula = AvgQuote_2015 ~ ., data=myBDF) avgq2015pred <- (predict(avgq2015RPART, newdata=myBDF)) myBDF$AvgQuote_2015[is.na(myBDF$AvgQuote_2015)] <- avgq2015pred[is.na(myBDF$AvgQuote_2015)] # 2016 avgq2016RPART <- rpart(formula = AvgQuote_2016 ~ ., data=myBDF) avgq2016pred <- (predict(avgq2016RPART, newdata=myBDF)) myBDF$AvgQuote_2016[is.na(myBDF$AvgQuote_2016)] <- avgq2016pred[is.na(myBDF$AvgQuote_2016)] # 2017 avgq2017RPART <- rpart(formula = AvgQuote_2017 ~ ., data=myBDF) avgq2017pred <- (predict(avgq2017RPART, newdata=myBDF)) myBDF$AvgQuote_2017[is.na(myBDF$AvgQuote_2017)] <- avgq2017pred[is.na(myBDF$AvgQuote_2017)] # 2018 avgq2018RPART <- rpart(formula = AvgQuote_2018 ~ ., data=myBDF) avgq2018pred <- (predict(avgq2018RPART, newdata=myBDF)) myBDF$AvgQuote_2018[is.na(myBDF$AvgQuote_2018)] <- avgq2018pred[is.na(myBDF$AvgQuote_2018)] # TIV # 2015 TIV2015RPART <- rpart(formula = AvgTIV_2015 ~ ., data=myBDF) TIV2015pred <- round(predict(TIV2015RPART, newdata=myBDF)) myBDF$AvgTIV_2015[is.na(myBDF$AvgTIV_2015)] <- TIV2015pred[is.na(myBDF$AvgTIV_2015)] # 2016 TIV2016RPART <- rpart(formula = AvgTIV_2016 ~ ., data=myBDF) TIV2016pred <- round(predict(TIV2016RPART, newdata=myBDF)) myBDF$AvgTIV_2016[is.na(myBDF$AvgTIV_2016)] <- TIV2016pred[is.na(myBDF$AvgTIV_2016)] # 2017 TIV2017RPART <- rpart(formula = AvgTIV_2017 ~ ., data=myBDF) TIV2017pred <- round(predict(TIV2017RPART, newdata=myBDF)) myBDF$AvgTIV_2017[is.na(myBDF$AvgTIV_2017)] <- TIV2017pred[is.na(myBDF$AvgTIV_2017)] # 2018 TIV2018RPART <- rpart(formula = AvgTIV_2018 ~ ., data=myBDF) TIV2018pred <- round(predict(TIV2018RPART, newdata=myBDF)) myBDF$AvgTIV_2018[is.na(myBDF$AvgTIV_2018)] <- TIV2018pred[is.na(myBDF$AvgTIV_2018)] # GWP # 2015 GWP2015RPART <- rpart(formula = GWP_2015 ~ ., data=myBDF) GWP2015pred <- (predict(GWP2015RPART, newdata=myBDF)) myBDF$GWP_2015[is.na(myBDF$GWP_2015)] <- GWP2015pred[is.na(myBDF$GWP_2015)] # 2016 GWP2016RPART <- rpart(formula = GWP_2016 ~ ., data=myBDF) GWP2016pred <- (predict(GWP2016RPART, newdata=myBDF)) myBDF$GWP_2016[is.na(myBDF$GWP_2016)] <- GWP2016pred[is.na(myBDF$GWP_2016)] # 2017 GWP2017RPART <- rpart(formula = GWP_2017 ~ ., data=myBDF) GWP2017pred <- (predict(GWP2017RPART, newdata=myBDF)) myBDF$GWP_2017[is.na(myBDF$GWP_2017)] <- GWP2017pred[is.na(myBDF$GWP_2017)] # 2018 GWP2018RPART <- rpart(formula = GWP_2018 ~ ., data=myBDF) GWP2018pred <- (predict(GWP2018RPART, newdata=myBDF)) myBDF$GWP_2018[is.na(myBDF$GWP_2018)] <- GWP2018pred[is.na(myBDF$GWP_2018)]","title":"Handling missing values"},{"location":"Clustering/","text":"Distance Based Clustering All the variables differ in scale which can be verified using: summary(myalchemySubset), hence we need to scale (divide by standard deviation) all variables, this can be achieved using the function named \u201cscale\u201d. # To center and scale the data (normalization) brokerHCDF <- scale(myalchemySubset, center=TRUE, scale=TRUE) head(brokerHCDF) ## QuoteCount_2016 QuoteCount_2017 QuoteCount_2018 PolicyCount_2016 ## [1,] -0.2955356 -0.4048377 -0.3796046 0.14470752 ## [2,] -0.4577748 -0.5737336 -0.4885734 -0.07385284 ## [3,] -0.5925581 0.1518932 -0.2306064 -0.16587826 ## [4,] -0.1258084 -0.5706059 0.7590080 -0.61450215 ## [5,] -0.5626063 5.3282393 0.4009677 0.63934411 ## [6,] -0.1907041 -0.1577493 -0.2995459 0.07568846 ## PolicyCount_2017 PolicyCount_2018 GWP_2016 GWP_2017 GWP_2018 ## [1,] 0.02015343 0.01544948 0.01109877 -0.1516617 -0.18327433 ## [2,] -0.13631801 -0.10511479 -0.12787748 -0.1833457 -0.02816983 ## [3,] -0.29278945 -0.18183751 -0.23868017 -0.2451593 -0.32193001 ## [4,] -0.62808540 -0.63121344 -0.65597316 -0.5389464 -0.60948072 ## [5,] 0.41133203 0.77171629 -0.23868017 -0.2451593 -0.06645961 ## [6,] -0.06925882 -0.12703557 0.04796681 -0.2655789 -0.22890709 ## success_ratio16_18 ## [1,] -0.3583157 ## [2,] -0.5772911 ## [3,] 0.1466954 ## [4,] -0.1856545 ## [5,] 3.2993371 ## [6,] -0.3972832 Hierarchical Clustering Clustering can be achieved using either distance based or density based methods. In this case, distance based clustering methods will work best . We can segment the brokers into 5 segments using two distance based clustering methods: The hierarchical clustering method and the K-means clustering method. In the hierarchical clustering approach, we calculate the pairwise (euclidean) distance between the observations using the function \u201cdist\u201d and then cluster these using \u201chclust\u201d function. Subsequently, we need to find the number of clusters to which each broker belongs using the \u201ccutree\u201d function. #Calculating pairwise distances between observations (default dist() function measures euclidean distance) myHCDist <- dist(brokerHCDF) head(myHCDist) ## [1] 0.4958177 0.9995419 1.8904891 6.9268765 0.3597904 0.3081979 Histogram of the distances (to help define a point's \"neighborhood\") hist(myHCDist) Using hclust function to do hierachical clustering- from the dendrogram, the height of the lines indicate distance between clusters brokerHclust <- hclust(myHCDist) plot(brokerHclust) Obtaining cluster membership using cutree function- The numbers show the cluster to which each broker belongs brokerCut <- cutree(brokerHclust, k=5) # 5 best clusters brokerCut[1:20] ## [1] 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 2 2 1 1 1 Further, in order to visualize the clusters, we do the principal component analysis. A silhouette plot (plot(brokerSil) shows how well each point fits to its cluster. Principal Component Analysis #PCA # Plotting first two PC scores to visually evaluate the quality of the clusters- longer bar to the right indicates stronger cluster membership. brokerHCPCA <- prcomp(brokerHCDF, retx=TRUE) plot(brokerHCPCA$x[,1:2], col=brokerCut, pch=brokerCut) legend(\"topright\", title= \"Alchemy -Broker Seg.- Hierarchical Clustering \", legend = 1:5, col = 1:5, pch = 1:5) The quality of the cluster groups can be visually evaluated with the PCA plot from the first two PCs. Most clusters appear well defined and can be clearly distinguished. Cluster 1 (black circles) and Cluster 2 (red triangles) seem more prominent. Cluster 4 (blue x) looks to slightly overlap with Cluster 1 . A few outliers were spotted that needed to be further examined. # Looking for \"elbow\" in plot screeplot(brokerHCPCA) The visual representation of the variance can be explained through the \u201cscree plot\u201d. The difference appears to gradually decrease after the first principal component, thus differences in the data are not captured only by the first few principal components. The 3rd and the 4th PCs need to be visualized as well. # Silhouette plot to see how well each plot belongs to each cluster brokerSil <- silhouette(brokerCut, myHCDist) plot(brokerSil) The silhouette plot for hierarchical clustering displayed well clustered groups for the most part. All points for Cluster 3 , Cluster 4 , and Cluster 5 appear well grouped. Cluster 1 contains a small number of points with negative coefficients. Most points for Cluster 2 are well clustered. However, there is a larger number of points with negative coefficients which implies that the points could have probably been assigned to a different cluster. The average silhouette coefficient appears good with a value of 0.55 . Outlier detection and removal which(brokerHCPCA$x[,1] > 10) ## [1] 76 myalchemySubset <- myalchemySubset[-76,] K-means Clustering Using k-means clustering approach to segment brokers we can use the function \u201ckmeans\u201d. To understand the presented patterns and be able to identify characteristics of each cluster group we need to evaluate the rotation matrix using the function \u201crotation\u201d. # Creating broker segments in to 5 groups using k-means clustering brokerKmeans <- kmeans(brokerHCDF, centers=5) # Plotting the clusters using PCA brokerHCPCA <- prcomp(brokerHCDF, retx = TRUE) plot(brokerHCPCA$x[,1:2], col=brokerKmeans$cluster, pch=brokerKmeans$cluster) legend(\"topright\", title= \"Alchemy -Broker's Seg.- K-means Clustering\", legend = 1:5, col = 1:5, pch = 1:5) # Plotting each individual cluster to help determine characteristics - (update cluster # to the one to be analyzed) for (i in 1:5) { plot(brokerHCPCA$x[,1:2], type=\"n\") points(brokerHCPCA$x[brokerKmeans$cluster==i, 1:2], col = brokerKmeans$cluster[brokerKmeans$cluster== i], pch = brokerKmeans$cluster[brokerKmeans$cluster== i]) } The PCA plot to evaluate the quality of the cluster groups obtained with the K-means clustering approach can be observed above. The plot shows a total of five cluster groups with different sizes and densities. Clusters appear better assigned and well separated in comparison to the hierarchical clustering approach. # K-means clustering with 5 clusters of sizes 131, 29, 12, 13, 3 # # Cluster means: # QuoteCount_2016 QuoteCount_2017 QuoteCount_2018 PolicyCount_2016 # 1 -0.3068342 -0.3781926 -0.22556457 -0.4130648 # 2 0.9022804 0.1701201 0.15181956 0.5873815 # 3 0.4405728 3.0575279 0.08573662 0.2281055 # 4 0.7015156 0.7560208 0.38591397 2.7833593 # 5 -0.1258084 -0.6362876 6.36682337 -0.6145021 # PolicyCount_2017 PolicyCount_2018 GWP_2016 GWP_2017 GWP_2018 # 1 -0.4919194 -0.4874734 -0.3810617 -0.4231120 -0.4240929 # 2 0.9913555 0.9448148 0.4729498 0.7017169 0.7489131 # 3 0.2371644 0.2903726 -0.1027550 -0.1784607 -0.2087987 # 4 2.6715706 2.6821963 3.0311171 2.9874035 2.9362854 # 5 -0.6280854 -0.6312134 -0.6559732 -0.5389464 -0.6094807 # success_ratio16_18 # 1 -0.4800682 # 2 0.5180605 # 3 1.5806917 # 4 1.3770743 # 5 3.6649698 # # Clustering vector: # [1] 1 1 1 1 3 1 1 1 1 4 1 1 1 2 1 2 1 1 1 2 1 3 1 1 1 1 1 1 1 4 1 1 1 2 1 2 1 # [38] 1 1 1 1 1 1 4 1 1 4 1 1 2 1 1 3 1 1 1 4 1 1 1 1 1 3 1 1 1 4 1 2 2 1 1 1 2 # [75] 2 4 3 1 4 2 5 1 1 2 1 1 2 2 4 2 1 1 1 1 1 1 2 1 1 4 1 2 1 3 1 1 4 1 1 1 1 # [112] 1 1 2 1 1 1 2 1 1 3 1 1 2 1 3 1 2 1 1 1 1 2 1 1 5 1 1 2 1 3 1 1 1 3 1 1 1 # [149] 2 1 1 1 3 1 2 1 3 1 1 1 1 1 1 2 1 1 1 4 1 1 1 1 1 1 2 1 1 2 4 1 2 1 1 1 1 # [186] 1 1 5 # # Within cluster sum of squares by cluster: # [1] 104.96338 209.65115 83.34065 107.16159 18.55363 # (between_SS / total_SS = 72.0 %) # # Available components: # # [1] \"cluster\" \"centers\" \"totss\" \"withinss\" \"tot.withinss\" # [6] \"betweenss\" \"size\" \"iter\" \"ifault\" # Determining which variables are responsible for the variation in the data summary(brokerHCPCA) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 2.4910 1.2507 1.0196 0.89092 0.41818 0.37359 0.20226 ## Proportion of Variance 0.6205 0.1564 0.1040 0.07937 0.01749 0.01396 0.00409 ## Cumulative Proportion 0.6205 0.7770 0.8809 0.96028 0.97777 0.99172 0.99582 ## PC8 PC9 PC10 ## Standard deviation 0.14980 0.12591 0.05956 ## Proportion of Variance 0.00224 0.00159 0.00035 ## Cumulative Proportion 0.99806 0.99965 1.00000 # Using principal component analysis to identify characteristics of each cluster. brokerHCPCA$rotation ## PC1 PC2 PC3 PC4 PC5 ## QuoteCount_2016 0.1737438 0.23802162 0.697357246 0.49459120 0.28992197 ## QuoteCount_2017 0.2037561 0.33872873 0.340337733 -0.73523924 -0.18830152 ## QuoteCount_2018 0.1063760 0.60339350 -0.501663481 0.34073673 -0.07260356 ## PolicyCount_2016 0.3638513 -0.15232670 -0.205894098 -0.18788011 0.51464891 ## PolicyCount_2017 0.3910313 -0.05818996 0.106782482 0.07607014 0.06119190 ## PolicyCount_2018 0.3899349 -0.04123842 0.118105591 0.05992944 -0.05757329 ## GWP_2016 0.3480959 -0.25972425 -0.233098566 -0.06757504 0.37727784 ## GWP_2017 0.3753182 -0.19987459 -0.069909679 0.11330363 -0.34794530 ## GWP_2018 0.3712609 -0.18390788 -0.009658718 0.15237555 -0.57826904 ## success_ratio16_18 0.2824341 0.54314464 -0.138338160 -0.11576587 0.07412351 ## PC6 PC7 PC8 PC9 PC10 ## QuoteCount_2016 -0.24294391 0.1753603 -0.06534234 0.05455046 -0.03403150 ## QuoteCount_2017 -0.18358516 0.1308877 -0.17350971 -0.26595951 -0.01439228 ## QuoteCount_2018 -0.05820048 0.1760774 -0.23648449 -0.39895022 -0.04565784 ## PolicyCount_2016 0.26841494 0.5002830 -0.33093017 0.25660261 -0.06453389 ## PolicyCount_2017 0.37317332 -0.3850655 -0.17025414 -0.26330250 0.66100408 ## PolicyCount_2018 0.46503105 -0.1547210 0.29875586 -0.28475549 -0.64409763 ## GWP_2016 -0.61482395 -0.1187774 0.35068592 -0.30703581 0.02764954 ## GWP_2017 -0.31125376 -0.3473289 -0.55428514 0.29626503 -0.26052658 ## GWP_2018 -0.03443796 0.5401697 0.32270406 0.09128358 0.25533623 ## success_ratio16_18 0.01597787 -0.2684920 0.38417832 0.60041075 0.08223208 # Determining variables responsible for variation in the data brokerHCPCA$rotation[,1:3] The Importance of principal components demonstrates that the variance explained by the first principal component: 62.05% , the difference explained by the second principal component: 15.64% , and the variance described by the third component: 10.40% . The cumulative proportion of the difference described by the first two principal components was 77.70% . PC1 : The loadings obtained from the rotation matrix indicate that the first component appears to be associated with brokers with higher policy counts for 2017 and 2018 (with +.39 and +.38, respectively) and higher gross written premiums for 2017 and 2018 (with +.37). Agents with larger values on the first component will reveal large values for these variables. PC2 : The second principal component seem to relate to brokers with higher quote counts for 2018 (+.60) and higher success ratios between 2016 and 2018 (+.54). Agents with larger values on the second component will have large values for these variables. PC3 : The variation on the third principal component is a tradeoff between the number of quote counts for 2016 (+.69) and quote counts for 2018 (-.50). # PC1 PC2 PC3 # QuoteCount_2016 0.1737438 0.23802162 0.697357246 # QuoteCount_2017 0.2037561 0.33872873 0.340337733 # QuoteCount_2018 0.1063760 0.60339350 -0.501663481 # PolicyCount_2016 0.3638513 -0.15232670 -0.205894098 # PolicyCount_2017 0.3910313 -0.05818996 0.106782482 # PolicyCount_2018 0.3899349 -0.04123842 0.118105591 # GWP_2016 0.3480959 -0.25972425 -0.233098566 # GWP_2017 0.3753182 -0.19987459 -0.069909679 # GWP_2018 0.3712609 -0.18390788 -0.009658718 # success_ratio16_18 0.2824341 0.54314464 -0.138338160 brokerKmeansSil <- silhouette(brokerKmeans$cluster, myHCDist) plot(brokerKmeansSil) The silhouette plot obtained from the k-means clustering also shows well clustered groups. All points for Cluster 1 , Cluster 4 , and cluster 5 seem well grouped. Cluster 3 has few points with negative coefficients. Most points for Cluster 2 look nicely clustered. However, there are a number of points with negative coefficients which suggests that the points could have been allocated to another cluster. The average silhouette coefficient looks good with a value of 0.56 .","title":"Clustering & PCA"},{"location":"Clustering/#distance-based-clustering","text":"All the variables differ in scale which can be verified using: summary(myalchemySubset), hence we need to scale (divide by standard deviation) all variables, this can be achieved using the function named \u201cscale\u201d. # To center and scale the data (normalization) brokerHCDF <- scale(myalchemySubset, center=TRUE, scale=TRUE) head(brokerHCDF) ## QuoteCount_2016 QuoteCount_2017 QuoteCount_2018 PolicyCount_2016 ## [1,] -0.2955356 -0.4048377 -0.3796046 0.14470752 ## [2,] -0.4577748 -0.5737336 -0.4885734 -0.07385284 ## [3,] -0.5925581 0.1518932 -0.2306064 -0.16587826 ## [4,] -0.1258084 -0.5706059 0.7590080 -0.61450215 ## [5,] -0.5626063 5.3282393 0.4009677 0.63934411 ## [6,] -0.1907041 -0.1577493 -0.2995459 0.07568846 ## PolicyCount_2017 PolicyCount_2018 GWP_2016 GWP_2017 GWP_2018 ## [1,] 0.02015343 0.01544948 0.01109877 -0.1516617 -0.18327433 ## [2,] -0.13631801 -0.10511479 -0.12787748 -0.1833457 -0.02816983 ## [3,] -0.29278945 -0.18183751 -0.23868017 -0.2451593 -0.32193001 ## [4,] -0.62808540 -0.63121344 -0.65597316 -0.5389464 -0.60948072 ## [5,] 0.41133203 0.77171629 -0.23868017 -0.2451593 -0.06645961 ## [6,] -0.06925882 -0.12703557 0.04796681 -0.2655789 -0.22890709 ## success_ratio16_18 ## [1,] -0.3583157 ## [2,] -0.5772911 ## [3,] 0.1466954 ## [4,] -0.1856545 ## [5,] 3.2993371 ## [6,] -0.3972832","title":"Distance Based Clustering"},{"location":"Clustering/#hierarchical-clustering","text":"Clustering can be achieved using either distance based or density based methods. In this case, distance based clustering methods will work best . We can segment the brokers into 5 segments using two distance based clustering methods: The hierarchical clustering method and the K-means clustering method. In the hierarchical clustering approach, we calculate the pairwise (euclidean) distance between the observations using the function \u201cdist\u201d and then cluster these using \u201chclust\u201d function. Subsequently, we need to find the number of clusters to which each broker belongs using the \u201ccutree\u201d function. #Calculating pairwise distances between observations (default dist() function measures euclidean distance) myHCDist <- dist(brokerHCDF) head(myHCDist) ## [1] 0.4958177 0.9995419 1.8904891 6.9268765 0.3597904 0.3081979 Histogram of the distances (to help define a point's \"neighborhood\") hist(myHCDist) Using hclust function to do hierachical clustering- from the dendrogram, the height of the lines indicate distance between clusters brokerHclust <- hclust(myHCDist) plot(brokerHclust) Obtaining cluster membership using cutree function- The numbers show the cluster to which each broker belongs brokerCut <- cutree(brokerHclust, k=5) # 5 best clusters brokerCut[1:20] ## [1] 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 2 2 1 1 1 Further, in order to visualize the clusters, we do the principal component analysis. A silhouette plot (plot(brokerSil) shows how well each point fits to its cluster.","title":"Hierarchical Clustering"},{"location":"Clustering/#principal-component-analysis","text":"#PCA # Plotting first two PC scores to visually evaluate the quality of the clusters- longer bar to the right indicates stronger cluster membership. brokerHCPCA <- prcomp(brokerHCDF, retx=TRUE) plot(brokerHCPCA$x[,1:2], col=brokerCut, pch=brokerCut) legend(\"topright\", title= \"Alchemy -Broker Seg.- Hierarchical Clustering \", legend = 1:5, col = 1:5, pch = 1:5) The quality of the cluster groups can be visually evaluated with the PCA plot from the first two PCs. Most clusters appear well defined and can be clearly distinguished. Cluster 1 (black circles) and Cluster 2 (red triangles) seem more prominent. Cluster 4 (blue x) looks to slightly overlap with Cluster 1 . A few outliers were spotted that needed to be further examined. # Looking for \"elbow\" in plot screeplot(brokerHCPCA) The visual representation of the variance can be explained through the \u201cscree plot\u201d. The difference appears to gradually decrease after the first principal component, thus differences in the data are not captured only by the first few principal components. The 3rd and the 4th PCs need to be visualized as well. # Silhouette plot to see how well each plot belongs to each cluster brokerSil <- silhouette(brokerCut, myHCDist) plot(brokerSil) The silhouette plot for hierarchical clustering displayed well clustered groups for the most part. All points for Cluster 3 , Cluster 4 , and Cluster 5 appear well grouped. Cluster 1 contains a small number of points with negative coefficients. Most points for Cluster 2 are well clustered. However, there is a larger number of points with negative coefficients which implies that the points could have probably been assigned to a different cluster. The average silhouette coefficient appears good with a value of 0.55 . Outlier detection and removal which(brokerHCPCA$x[,1] > 10) ## [1] 76 myalchemySubset <- myalchemySubset[-76,]","title":"Principal Component Analysis"},{"location":"Clustering/#k-means-clustering","text":"Using k-means clustering approach to segment brokers we can use the function \u201ckmeans\u201d. To understand the presented patterns and be able to identify characteristics of each cluster group we need to evaluate the rotation matrix using the function \u201crotation\u201d. # Creating broker segments in to 5 groups using k-means clustering brokerKmeans <- kmeans(brokerHCDF, centers=5) # Plotting the clusters using PCA brokerHCPCA <- prcomp(brokerHCDF, retx = TRUE) plot(brokerHCPCA$x[,1:2], col=brokerKmeans$cluster, pch=brokerKmeans$cluster) legend(\"topright\", title= \"Alchemy -Broker's Seg.- K-means Clustering\", legend = 1:5, col = 1:5, pch = 1:5) # Plotting each individual cluster to help determine characteristics - (update cluster # to the one to be analyzed) for (i in 1:5) { plot(brokerHCPCA$x[,1:2], type=\"n\") points(brokerHCPCA$x[brokerKmeans$cluster==i, 1:2], col = brokerKmeans$cluster[brokerKmeans$cluster== i], pch = brokerKmeans$cluster[brokerKmeans$cluster== i]) } The PCA plot to evaluate the quality of the cluster groups obtained with the K-means clustering approach can be observed above. The plot shows a total of five cluster groups with different sizes and densities. Clusters appear better assigned and well separated in comparison to the hierarchical clustering approach. # K-means clustering with 5 clusters of sizes 131, 29, 12, 13, 3 # # Cluster means: # QuoteCount_2016 QuoteCount_2017 QuoteCount_2018 PolicyCount_2016 # 1 -0.3068342 -0.3781926 -0.22556457 -0.4130648 # 2 0.9022804 0.1701201 0.15181956 0.5873815 # 3 0.4405728 3.0575279 0.08573662 0.2281055 # 4 0.7015156 0.7560208 0.38591397 2.7833593 # 5 -0.1258084 -0.6362876 6.36682337 -0.6145021 # PolicyCount_2017 PolicyCount_2018 GWP_2016 GWP_2017 GWP_2018 # 1 -0.4919194 -0.4874734 -0.3810617 -0.4231120 -0.4240929 # 2 0.9913555 0.9448148 0.4729498 0.7017169 0.7489131 # 3 0.2371644 0.2903726 -0.1027550 -0.1784607 -0.2087987 # 4 2.6715706 2.6821963 3.0311171 2.9874035 2.9362854 # 5 -0.6280854 -0.6312134 -0.6559732 -0.5389464 -0.6094807 # success_ratio16_18 # 1 -0.4800682 # 2 0.5180605 # 3 1.5806917 # 4 1.3770743 # 5 3.6649698 # # Clustering vector: # [1] 1 1 1 1 3 1 1 1 1 4 1 1 1 2 1 2 1 1 1 2 1 3 1 1 1 1 1 1 1 4 1 1 1 2 1 2 1 # [38] 1 1 1 1 1 1 4 1 1 4 1 1 2 1 1 3 1 1 1 4 1 1 1 1 1 3 1 1 1 4 1 2 2 1 1 1 2 # [75] 2 4 3 1 4 2 5 1 1 2 1 1 2 2 4 2 1 1 1 1 1 1 2 1 1 4 1 2 1 3 1 1 4 1 1 1 1 # [112] 1 1 2 1 1 1 2 1 1 3 1 1 2 1 3 1 2 1 1 1 1 2 1 1 5 1 1 2 1 3 1 1 1 3 1 1 1 # [149] 2 1 1 1 3 1 2 1 3 1 1 1 1 1 1 2 1 1 1 4 1 1 1 1 1 1 2 1 1 2 4 1 2 1 1 1 1 # [186] 1 1 5 # # Within cluster sum of squares by cluster: # [1] 104.96338 209.65115 83.34065 107.16159 18.55363 # (between_SS / total_SS = 72.0 %) # # Available components: # # [1] \"cluster\" \"centers\" \"totss\" \"withinss\" \"tot.withinss\" # [6] \"betweenss\" \"size\" \"iter\" \"ifault\" # Determining which variables are responsible for the variation in the data summary(brokerHCPCA) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 2.4910 1.2507 1.0196 0.89092 0.41818 0.37359 0.20226 ## Proportion of Variance 0.6205 0.1564 0.1040 0.07937 0.01749 0.01396 0.00409 ## Cumulative Proportion 0.6205 0.7770 0.8809 0.96028 0.97777 0.99172 0.99582 ## PC8 PC9 PC10 ## Standard deviation 0.14980 0.12591 0.05956 ## Proportion of Variance 0.00224 0.00159 0.00035 ## Cumulative Proportion 0.99806 0.99965 1.00000 # Using principal component analysis to identify characteristics of each cluster. brokerHCPCA$rotation ## PC1 PC2 PC3 PC4 PC5 ## QuoteCount_2016 0.1737438 0.23802162 0.697357246 0.49459120 0.28992197 ## QuoteCount_2017 0.2037561 0.33872873 0.340337733 -0.73523924 -0.18830152 ## QuoteCount_2018 0.1063760 0.60339350 -0.501663481 0.34073673 -0.07260356 ## PolicyCount_2016 0.3638513 -0.15232670 -0.205894098 -0.18788011 0.51464891 ## PolicyCount_2017 0.3910313 -0.05818996 0.106782482 0.07607014 0.06119190 ## PolicyCount_2018 0.3899349 -0.04123842 0.118105591 0.05992944 -0.05757329 ## GWP_2016 0.3480959 -0.25972425 -0.233098566 -0.06757504 0.37727784 ## GWP_2017 0.3753182 -0.19987459 -0.069909679 0.11330363 -0.34794530 ## GWP_2018 0.3712609 -0.18390788 -0.009658718 0.15237555 -0.57826904 ## success_ratio16_18 0.2824341 0.54314464 -0.138338160 -0.11576587 0.07412351 ## PC6 PC7 PC8 PC9 PC10 ## QuoteCount_2016 -0.24294391 0.1753603 -0.06534234 0.05455046 -0.03403150 ## QuoteCount_2017 -0.18358516 0.1308877 -0.17350971 -0.26595951 -0.01439228 ## QuoteCount_2018 -0.05820048 0.1760774 -0.23648449 -0.39895022 -0.04565784 ## PolicyCount_2016 0.26841494 0.5002830 -0.33093017 0.25660261 -0.06453389 ## PolicyCount_2017 0.37317332 -0.3850655 -0.17025414 -0.26330250 0.66100408 ## PolicyCount_2018 0.46503105 -0.1547210 0.29875586 -0.28475549 -0.64409763 ## GWP_2016 -0.61482395 -0.1187774 0.35068592 -0.30703581 0.02764954 ## GWP_2017 -0.31125376 -0.3473289 -0.55428514 0.29626503 -0.26052658 ## GWP_2018 -0.03443796 0.5401697 0.32270406 0.09128358 0.25533623 ## success_ratio16_18 0.01597787 -0.2684920 0.38417832 0.60041075 0.08223208 # Determining variables responsible for variation in the data brokerHCPCA$rotation[,1:3] The Importance of principal components demonstrates that the variance explained by the first principal component: 62.05% , the difference explained by the second principal component: 15.64% , and the variance described by the third component: 10.40% . The cumulative proportion of the difference described by the first two principal components was 77.70% . PC1 : The loadings obtained from the rotation matrix indicate that the first component appears to be associated with brokers with higher policy counts for 2017 and 2018 (with +.39 and +.38, respectively) and higher gross written premiums for 2017 and 2018 (with +.37). Agents with larger values on the first component will reveal large values for these variables. PC2 : The second principal component seem to relate to brokers with higher quote counts for 2018 (+.60) and higher success ratios between 2016 and 2018 (+.54). Agents with larger values on the second component will have large values for these variables. PC3 : The variation on the third principal component is a tradeoff between the number of quote counts for 2016 (+.69) and quote counts for 2018 (-.50). # PC1 PC2 PC3 # QuoteCount_2016 0.1737438 0.23802162 0.697357246 # QuoteCount_2017 0.2037561 0.33872873 0.340337733 # QuoteCount_2018 0.1063760 0.60339350 -0.501663481 # PolicyCount_2016 0.3638513 -0.15232670 -0.205894098 # PolicyCount_2017 0.3910313 -0.05818996 0.106782482 # PolicyCount_2018 0.3899349 -0.04123842 0.118105591 # GWP_2016 0.3480959 -0.25972425 -0.233098566 # GWP_2017 0.3753182 -0.19987459 -0.069909679 # GWP_2018 0.3712609 -0.18390788 -0.009658718 # success_ratio16_18 0.2824341 0.54314464 -0.138338160 brokerKmeansSil <- silhouette(brokerKmeans$cluster, myHCDist) plot(brokerKmeansSil) The silhouette plot obtained from the k-means clustering also shows well clustered groups. All points for Cluster 1 , Cluster 4 , and cluster 5 seem well grouped. Cluster 3 has few points with negative coefficients. Most points for Cluster 2 look nicely clustered. However, there are a number of points with negative coefficients which suggests that the points could have been allocated to another cluster. The average silhouette coefficient looks good with a value of 0.56 .","title":"K-means Clustering"},{"location":"EDA/","text":"Exploratory Data Analysis We can generate additional metrics to measure broker performance: \"Hit Ratio\", \"Quote Ratio\" & \"Success Ratio\" Let's create a second data frame called \u201cmyBDFclean\u201d. In this data frame variables pertaining to the years 2013 and 2014 need to be excluded. Furthermore, this data frame can be used to generate additional features/ calculations to measure broker performance using the function \u201cmutate\u201d. The new calculated fields include supplementary operational metrics such as, \u201cQuote Ratio\u201d for year x : Quote count for the year x/ Submissions for the year x, \u201cHit Ratio\u201d for year x : Policy count for the year x/ Quote count for the year x \u201cSuccess Ratio\u201d for years x....n : sum(Policy count for year x...n)/ sum(Submissions for year x...n) With respect to the broker segmentation task, we need to create a subset named; \u201cmyalchemySubset\u201d from the data frame \u201cmyBDFClean\u201d. This subset includes three years\u2019 worth of data required for the clustering. The variables chosen for part 1 of the project were, quote counts: 'QuoteCount' from 2016 to 2018, policy counts: 'PolicyCount' from 2016 to 2018, gross written premiums: GWP from 2016 to 2018 as well as the newly generated measure; success ratio (from 2016 to 2018). # To remove unnecessary columns before clustering creating new DF only for clustering myBDFclean <- myBDF %>% mutate(quote_ratio2016 = QuoteCount_2016/ Submissions_2016, quote_ratio2017 = QuoteCount_2017/ Submissions_2017, quote_ratio2018 = QuoteCount_2018/ Submissions_2018, hit_ratio16 = PolicyCount_2016/ QuoteCount_2016, hit_ratio17 = PolicyCount_2017/ QuoteCount_2017, hit_ratio18 = PolicyCount_2018/ QuoteCount_2018, success_ratio16_18 = PolicyCount_2016+ PolicyCount_2017 + PolicyCount_2018/ Submissions_2016 + Submissions_2017 + Submissions_2018) %>% select(-Submissions_2014, -QuoteCount_2013, -QuoteCount_2014, -AvgQuote_2013, -AvgQuote_2014) # Calculating NAs = 0 sum(is.na(myBDFclean)) ## [1] 0 Correlation Graphs To get a deeper insight into the data we can generate a series correlation graphs using the 'ggcorr' function of the 'Ggally' library # Plotting correlation plots with GWP (2016 to 2018) against all other variables ggcorr(myBDFclean) ggpairs(myBDFclean, columns = c(\"GWP_2018\", \"GWP_2017\", \"GWP_2016\",\"hit_ratio18\", \"hit_ratio17\", \"hit_ratio16\" )) ggpairs(myBDFclean, columns = c(\"GWP_2018\", \"GWP_2017\", \"GWP_2016\", \"quote_ratio2018\", \"quote_ratio2017\", \"quote_ratio2016\")) ggpairs(myBDFclean, columns = c(\"GWP_2018\", \"GWP_2017\", \"GWP_2016\", \"PolicyCount_2018\", \"PolicyCount_2017\", \"PolicyCount_2016\")) ggpairs(myBDFclean, columns = c(\"GWP_2018\", \"GWP_2017\", \"GWP_2016\", \"success_ratio16_18\")) ggpairs(myBDFclean, columns = c(\"GWP_2018\", \"GWP_2017\", \"GWP_2016\", \"QuoteCount_2018\", \"QuoteCount_2017\", \"QuoteCount_2016\")) ggpairs(myBDFclean, columns = c(\"GWP_2018\", \"GWP_2017\", \"GWP_2016\", \"Submissions_2018\", \"Submissions_2017\", \"Submissions_2016\")) ggpairs(myBDFclean, columns = c(\"GWP_2018\", \"GWP_2017\", \"GWP_2016\", \"AvgQuote_2018\", \"AvgQuote_2017\", \"AvgQuote_2016\")) ggpairs(myBDFclean, columns = c(\"GWP_2018\", \"GWP_2017\", \"GWP_2016\", \"AvgTIV_2018\", \"AvgTIV_2017\", \"AvgTIV_2016\")) Summary Statistics # Based on correlation graphs creating a subset of all important variables for clustering myalchemySubset <- (myBDFclean [, c(\"QuoteCount_2016\", \"QuoteCount_2017\", \"QuoteCount_2018\", \"PolicyCount_2016\", \"PolicyCount_2017\", \"PolicyCount_2018\", \"GWP_2016\", \"GWP_2017\", \"GWP_2018\", \"success_ratio16_18\")]) summary(myalchemySubset) ## QuoteCount_2016 QuoteCount_2017 QuoteCount_2018 PolicyCount_2016 ## Min. : 1.0 Min. : 1.0 Min. : 1.0 Min. : 1.00 ## 1st Qu.: 60.0 1st Qu.: 52.0 1st Qu.: 77.0 1st Qu.: 37.00 ## Median : 123.0 Median : 108.5 Median : 137.5 Median : 60.50 ## Mean : 240.4 Mean : 222.4 Mean : 259.7 Mean : 90.42 ## 3rd Qu.: 221.5 3rd Qu.: 254.5 3rd Qu.: 252.5 3rd Qu.: 94.75 ## Max. :2789.0 Max. :2063.0 Max. :4216.0 Max. :482.00 ## PolicyCount_2017 PolicyCount_2018 GWP_2016 GWP_2017 ## Min. : 15.0 Min. : 6.00 Min. : 4553 Min. : 267093 ## 1st Qu.: 43.0 1st Qu.: 41.00 1st Qu.: 473456 1st Qu.: 1155401 ## Median : 62.0 Median : 62.00 Median : 1476275 Median : 1698394 ## Mean : 99.2 Mean : 98.59 Mean : 2275913 Mean : 2715892 ## 3rd Qu.:125.8 3rd Qu.:122.75 3rd Qu.: 2247211 3rd Qu.: 2700402 ## Max. :466.0 Max. :475.00 Max. :14503932 Max. :18815197 ## GWP_2018 success_ratio16_18 ## Min. : 230565 Min. : 98.33 ## 1st Qu.: 861229 1st Qu.: 334.37 ## Median : 1692199 Median : 528.36 ## Mean : 2747650 Mean : 836.94 ## 3rd Qu.: 3140648 3rd Qu.:1042.60 ## Max. :21032163 Max. :4932.15","title":"Data Analysis"},{"location":"EDA/#exploratory-data-analysis","text":"We can generate additional metrics to measure broker performance: \"Hit Ratio\", \"Quote Ratio\" & \"Success Ratio\" Let's create a second data frame called \u201cmyBDFclean\u201d. In this data frame variables pertaining to the years 2013 and 2014 need to be excluded. Furthermore, this data frame can be used to generate additional features/ calculations to measure broker performance using the function \u201cmutate\u201d. The new calculated fields include supplementary operational metrics such as, \u201cQuote Ratio\u201d for year x : Quote count for the year x/ Submissions for the year x, \u201cHit Ratio\u201d for year x : Policy count for the year x/ Quote count for the year x \u201cSuccess Ratio\u201d for years x....n : sum(Policy count for year x...n)/ sum(Submissions for year x...n) With respect to the broker segmentation task, we need to create a subset named; \u201cmyalchemySubset\u201d from the data frame \u201cmyBDFClean\u201d. This subset includes three years\u2019 worth of data required for the clustering. The variables chosen for part 1 of the project were, quote counts: 'QuoteCount' from 2016 to 2018, policy counts: 'PolicyCount' from 2016 to 2018, gross written premiums: GWP from 2016 to 2018 as well as the newly generated measure; success ratio (from 2016 to 2018). # To remove unnecessary columns before clustering creating new DF only for clustering myBDFclean <- myBDF %>% mutate(quote_ratio2016 = QuoteCount_2016/ Submissions_2016, quote_ratio2017 = QuoteCount_2017/ Submissions_2017, quote_ratio2018 = QuoteCount_2018/ Submissions_2018, hit_ratio16 = PolicyCount_2016/ QuoteCount_2016, hit_ratio17 = PolicyCount_2017/ QuoteCount_2017, hit_ratio18 = PolicyCount_2018/ QuoteCount_2018, success_ratio16_18 = PolicyCount_2016+ PolicyCount_2017 + PolicyCount_2018/ Submissions_2016 + Submissions_2017 + Submissions_2018) %>% select(-Submissions_2014, -QuoteCount_2013, -QuoteCount_2014, -AvgQuote_2013, -AvgQuote_2014) # Calculating NAs = 0 sum(is.na(myBDFclean)) ## [1] 0","title":"Exploratory Data Analysis"},{"location":"EDA/#correlation-graphs","text":"To get a deeper insight into the data we can generate a series correlation graphs using the 'ggcorr' function of the 'Ggally' library # Plotting correlation plots with GWP (2016 to 2018) against all other variables ggcorr(myBDFclean) ggpairs(myBDFclean, columns = c(\"GWP_2018\", \"GWP_2017\", \"GWP_2016\",\"hit_ratio18\", \"hit_ratio17\", \"hit_ratio16\" )) ggpairs(myBDFclean, columns = c(\"GWP_2018\", \"GWP_2017\", \"GWP_2016\", \"quote_ratio2018\", \"quote_ratio2017\", \"quote_ratio2016\")) ggpairs(myBDFclean, columns = c(\"GWP_2018\", \"GWP_2017\", \"GWP_2016\", \"PolicyCount_2018\", \"PolicyCount_2017\", \"PolicyCount_2016\")) ggpairs(myBDFclean, columns = c(\"GWP_2018\", \"GWP_2017\", \"GWP_2016\", \"success_ratio16_18\")) ggpairs(myBDFclean, columns = c(\"GWP_2018\", \"GWP_2017\", \"GWP_2016\", \"QuoteCount_2018\", \"QuoteCount_2017\", \"QuoteCount_2016\")) ggpairs(myBDFclean, columns = c(\"GWP_2018\", \"GWP_2017\", \"GWP_2016\", \"Submissions_2018\", \"Submissions_2017\", \"Submissions_2016\")) ggpairs(myBDFclean, columns = c(\"GWP_2018\", \"GWP_2017\", \"GWP_2016\", \"AvgQuote_2018\", \"AvgQuote_2017\", \"AvgQuote_2016\")) ggpairs(myBDFclean, columns = c(\"GWP_2018\", \"GWP_2017\", \"GWP_2016\", \"AvgTIV_2018\", \"AvgTIV_2017\", \"AvgTIV_2016\"))","title":"Correlation Graphs"},{"location":"EDA/#summary-statistics","text":"# Based on correlation graphs creating a subset of all important variables for clustering myalchemySubset <- (myBDFclean [, c(\"QuoteCount_2016\", \"QuoteCount_2017\", \"QuoteCount_2018\", \"PolicyCount_2016\", \"PolicyCount_2017\", \"PolicyCount_2018\", \"GWP_2016\", \"GWP_2017\", \"GWP_2018\", \"success_ratio16_18\")]) summary(myalchemySubset) ## QuoteCount_2016 QuoteCount_2017 QuoteCount_2018 PolicyCount_2016 ## Min. : 1.0 Min. : 1.0 Min. : 1.0 Min. : 1.00 ## 1st Qu.: 60.0 1st Qu.: 52.0 1st Qu.: 77.0 1st Qu.: 37.00 ## Median : 123.0 Median : 108.5 Median : 137.5 Median : 60.50 ## Mean : 240.4 Mean : 222.4 Mean : 259.7 Mean : 90.42 ## 3rd Qu.: 221.5 3rd Qu.: 254.5 3rd Qu.: 252.5 3rd Qu.: 94.75 ## Max. :2789.0 Max. :2063.0 Max. :4216.0 Max. :482.00 ## PolicyCount_2017 PolicyCount_2018 GWP_2016 GWP_2017 ## Min. : 15.0 Min. : 6.00 Min. : 4553 Min. : 267093 ## 1st Qu.: 43.0 1st Qu.: 41.00 1st Qu.: 473456 1st Qu.: 1155401 ## Median : 62.0 Median : 62.00 Median : 1476275 Median : 1698394 ## Mean : 99.2 Mean : 98.59 Mean : 2275913 Mean : 2715892 ## 3rd Qu.:125.8 3rd Qu.:122.75 3rd Qu.: 2247211 3rd Qu.: 2700402 ## Max. :466.0 Max. :475.00 Max. :14503932 Max. :18815197 ## GWP_2018 success_ratio16_18 ## Min. : 230565 Min. : 98.33 ## 1st Qu.: 861229 1st Qu.: 334.37 ## Median : 1692199 Median : 528.36 ## Mean : 2747650 Mean : 836.94 ## 3rd Qu.: 3140648 3rd Qu.:1042.60 ## Max. :21032163 Max. :4932.15","title":"Summary Statistics"},{"location":"Prediction/","text":"GWP 2018 Prediction For prediction of Gross Written Premium for 2019 we need to first predict for 2018 to test our model accuracy. Since we are primarily interested in determing whether GWP increases or decreases for the upcoming year for each broker, a new column called \"Up_Down\" can be created that compares GWP 2018 values to GWP 2017 and labels the values as \"Up\" if GWP for 2018 exceeds 2017 and \"Down\" if GWP for 2018 is lower than 2017. The response variable is a factor (character variable), we have to use machine learning algorithms such as classification trees, logistic regression, etc. The new dataframe: \u201cmyBroker_exp_DF\u201d consists of all variables from 2015 to 2017. Variables from years 2013 and 2014 are excluded, and the new variable: \"Up_Down\" is added. # Prediction of GWP2018 set.seed(12345) Up_Down <- character(length(myBDF$GWP_2018)) Up_Down[myBDF$GWP_2017 < myBDF$GWP_2018] <- \"Up\" Up_Down[myBDF$GWP_2017 >= myBDF$GWP_2018] <- \"Down\" myBroker_exp_DF <- myBDF %>% dplyr::mutate(quote_ratio2015 = QuoteCount_2015/ Submissions_2015, quote_ratio2016 = QuoteCount_2016/ Submissions_2016, quote_ratio2017 = QuoteCount_2017/ Submissions_2017, hit_ratio15 = PolicyCount_2015/ QuoteCount_2015, hit_ratio16 = PolicyCount_2016/ QuoteCount_2016, hit_ratio17 = PolicyCount_2017/ QuoteCount_2017, success_ratio15_17 = PolicyCount_2015+ PolicyCount_2016 + PolicyCount_2017/ Submissions_2015 + Submissions_2016 + Submissions_2017, Up_Down) %>% select(-GWP_2018, -Submissions_2018, -QuoteCount_2018, -PolicyCount_2018, -AvgQuote_2018, -AvgTIV_2018, -Submissions_2014, -QuoteCount_2013, -QuoteCount_2014, -AvgQuote_2013, -AvgQuote_2014) myBroker_exp_DF$Up_Down <- as.factor(myBroker_exp_DF$Up_Down) myBrokerDF <- myBroker_exp_DF colnames(myBrokerDF) <- c(\"Submissions_3\", \"Submissions_2\", \"Submissions_1\", \"QuoteCount_3\",\"QuoteCount_2\", \"QuoteCount_1\", \"AvgQuote_3\", \"AvgQuote_2\", \"AvgQuote_1\", \"PolicyCount_3\", \"PolicyCount_2\", \"PolicyCount_1\", \"GWP_3\", \"GWP_2\", \"GWP_1\", \"AvgTIV_3\", \"AvgTIV_2\", \"AvgTIV1\", \"QR3\", \"QR2\", \"QR1\", \"HR3\", \"HR2\", \"HR1\",\"SR\",\"Up_Down\") We then proceed to partition the dataset into test (20%) and training (80%) using the function \u201ccreateDataPartition\u201d (found in the \"caret\" package). # Partitioning into training and test, training = 80% as the dataset is small trainRows <- createDataPartition(Up_Down, p = 0.8, list=FALSE) BrokTrainData18 <- myBrokerDF[trainRows,] BrokTestData18 <- myBrokerDF[-trainRows,] table(Up_Down) ## Up_Down ## Down Up ## 106 82 Classification Trees For the classification tree model, we use all variables in the training set to train the model. To train the model, we use the train function and save the results in \"\u201cmyRparttune\u201d, apply the \u201crpart\u201d method, and set the metric to \u201cROC\u201d to ensure the selection of best model. For tuning the model we need to include; tunelength = 10, default being 3, and split criteria set to \u201cinformation gain\u201d, default is Gini Index. The train function provided the best model at complexity parameter = 0.07. # Rpart # increasing tune length & split type as 'Entropy' splitEntropy = list(split = c(\"information\")) myRparttune <- train(Up_Down ~ ., data=BrokTrainData18, method=\"rpart\", metric=\"ROC\", tuneLength = 10, parms = splitEntropy, trControl=trainControl(classProbs=TRUE, summaryFunction=twoClassSummary)) plot(myRparttune) myRparttune$results ## cp ROC Sens Spec ROCSD SensSD SpecSD ## 1 0.00000000 0.7087193 0.7030466 0.6596959 0.06744125 0.1134558 0.1158620 ## 2 0.03535354 0.7025369 0.7053459 0.6661077 0.07400825 0.1096479 0.1015546 ## 3 0.07070707 0.6943522 0.7022634 0.6420807 0.07094654 0.1119068 0.1225912 ## 4 0.10606061 0.6882251 0.6545418 0.6915282 0.07224751 0.1549664 0.1601590 ## 5 0.14141414 0.6809741 0.6552857 0.6663226 0.08001456 0.1753808 0.1887295 ## 6 0.17676768 0.6757820 0.6695795 0.6547226 0.07962344 0.1698370 0.2027911 ## 7 0.21212121 0.6703244 0.6802461 0.6307015 0.08585172 0.1801601 0.2446506 ## 8 0.24747475 0.6499109 0.6265344 0.6665035 0.08323140 0.1763311 0.2818400 ## 9 0.28282828 0.6260870 0.6584177 0.5937563 0.09149758 0.1980458 0.3535952 ## 10 0.31818182 0.6106691 0.6954677 0.5258706 0.09481431 0.2145107 0.3797248 myRparttune$bestTune # best model with cp ## cp ## 1 0 myRparttune$finalModel ## n= 151 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 151 66 Down (0.56291391 0.43708609) ## 2) PolicyCount_1< 45.5 52 6 Down (0.88461538 0.11538462) * ## 3) PolicyCount_1>=45.5 99 39 Up (0.39393939 0.60606061) ## 6) AvgQuote_1< 30219.13 58 23 Down (0.60344828 0.39655172) ## 12) AvgTIV1>=7156371 29 5 Down (0.82758621 0.17241379) * ## 13) AvgTIV1< 7156371 29 11 Up (0.37931034 0.62068966) ## 26) HR2>=0.7449327 10 3 Down (0.70000000 0.30000000) * ## 27) HR2< 0.7449327 19 4 Up (0.21052632 0.78947368) * ## 7) AvgQuote_1>=30219.13 41 4 Up (0.09756098 0.90243902) * print(myRparttune) ## CART ## ## 151 samples ## 25 predictor ## 2 classes: 'Down', 'Up' ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 151, 151, 151, 151, 151, 151, ... ## Resampling results across tuning parameters: ## ## cp ROC Sens Spec ## 0.00000000 0.7087193 0.7030466 0.6596959 ## 0.03535354 0.7025369 0.7053459 0.6661077 ## 0.07070707 0.6943522 0.7022634 0.6420807 ## 0.10606061 0.6882251 0.6545418 0.6915282 ## 0.14141414 0.6809741 0.6552857 0.6663226 ## 0.17676768 0.6757820 0.6695795 0.6547226 ## 0.21212121 0.6703244 0.6802461 0.6307015 ## 0.24747475 0.6499109 0.6265344 0.6665035 ## 0.28282828 0.6260870 0.6584177 0.5937563 ## 0.31818182 0.6106691 0.6954677 0.5258706 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0. par(xpd = NA) plot(myRparttune$finalModel) text(myRparttune$finalModel, cex=.6) myRparttunepredtest <- predict(myRparttune, newdata=BrokTestData18) (myRparttunedConfusion <- table(BrokTestData18$Up_Down, myRparttunepredtest)) ## myRparttunepredtest ## Down Up ## Down 17 4 ## Up 3 13 1-sum(diag(myRparttunedConfusion))/sum(myRparttunedConfusion) ## [1] 0.1891892 myRparttunedpred <- predict(myRparttune, newdata=myBrokerDF) (myRparttunedCM <- table(myBrokerDF$Up_Down, myRparttunedpred)) ## myRparttunedpred ## Down Up ## Down 94 12 ## Up 17 65 1-sum(diag(myRparttunedCM))/sum(myRparttunedCM) ## [1] 0.1542553 myRparttunePredict <- predict(myRparttune, newdata=myBrokerDF, type=\"prob\") myRparttunePred <- prediction(myRparttunePredict[,2], myBrokerDF$Up_Down, label.ordering=c( \"Down\", \"Up\")) myRparttunePerf <- performance(myRparttunePred, \"tpr\", \"fpr\") performance(myRparttunePred, \"auc\") ## An object of class \"performance\" ## Slot \"x.name\": ## [1] \"None\" ## ## Slot \"y.name\": ## [1] \"Area under the ROC curve\" ## ## Slot \"alpha.name\": ## [1] \"none\" ## ## Slot \"x.values\": ## list() ## ## Slot \"y.values\": ## [[1]] ## [1] 0.8584906 ## ## ## Slot \"alpha.values\": ## list() The misclassification rate for the classification tree model when validating on the test data set (myRparttunepredtest) was 24.32 %. The accuracy rate for the model was 75.68%. The matrix represents a total of 13 observations correctly classified as \u201cUp\u201d and 15 observations correctly classified as \u201cDown\u201d. The misclassification rate for classification tree model when validated using the entire data set (myRparttunedpred) is 18.62 %. The accuracy rate for this model is 81.38%. The matrix shows a total of 68 observations correctly classified as \u201cUp\u201d and 85 observations correctly classified as \u201cDown\u201d. The variables that appear to be important for predicting the outcome for the gross written premium prediction for 2018 are Policy Counts, Average Quote, and Average Total Insured Value. Logistic Regression The logistic regression model using the training dataset (BrokTrainData18): Weights are not assigned as there is \u201clittle\u201d to \u201cno\u201d class imbalance (Up: 82, Dow: 106). To train the model, we use the train function (and store results in \u201cmyLRtrain\u201d), apply the \u201cglm\u201d method and set the metric to \u201cROC\u201d. # Logistic regression #The tuneLength parameter is used to determine the total number of combinations that will be evaluated myLRtrain <- train(Up_Down ~ ., data=BrokTrainData18, method=\"glm\", metric=\"ROC\", tuneLength = 10, trControl=trainControl(classProbs=TRUE, summaryFunction=twoClassSummary)) myLRtrain$results ## parameter ROC Sens Spec ROCSD SensSD SpecSD ## 1 none 0.7701864 0.7088419 0.6758822 0.06620168 0.09827614 0.1016044 myLRtrain$bestTune ## parameter ## 1 none myLRtrain$finalModel ## ## Call: NULL ## ## Coefficients: ## (Intercept) Submissions_3 Submissions_2 Submissions_1 QuoteCount_3 ## 9.596e-01 -5.909e-03 -3.232e+00 -3.234e+00 8.027e-03 ## QuoteCount_2 QuoteCount_1 AvgQuote_3 AvgQuote_2 AvgQuote_1 ## -1.148e-02 -5.399e-03 3.276e-05 -1.346e-05 2.173e-04 ## PolicyCount_3 PolicyCount_2 PolicyCount_1 GWP_3 GWP_2 ## -3.280e+00 -3.243e+00 7.454e-02 -6.097e-07 -6.275e-07 ## GWP_1 AvgTIV_3 AvgTIV_2 AvgTIV1 QR3 ## 4.961e-07 8.505e-07 -1.898e-07 -1.563e-06 -4.295e+00 ## QR2 QR1 HR3 HR2 HR1 ## 1.361e+00 2.944e-01 -7.731e-01 -1.279e-01 -1.896e+00 ## SR ## 3.239e+00 ## ## Degrees of Freedom: 150 Total (i.e. Null); 125 Residual ## Null Deviance: 206.9 ## Residual Deviance: 95.49 AIC: 147.5 print(myLRtrain) ## Generalized Linear Model ## ## 151 samples ## 25 predictor ## 2 classes: 'Down', 'Up' ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 151, 151, 151, 151, 151, 151, ... ## Resampling results: ## ## ROC Sens Spec ## 0.7701864 0.7088419 0.6758822 myLRtraintest <- predict(myLRtrain, newdata=BrokTestData18) (myLRtrainConfusion <- table(BrokTestData18$Up_Down, myLRtraintest)) ## myLRtraintest ## Down Up ## Down 15 6 ## Up 3 13 1-sum(diag(myLRtrainConfusion))/sum(myLRtrainConfusion) ## [1] 0.2432432 myLRtrainpred <- predict(myLRtrain, newdata=myBrokerDF) myLRtrainprob <- predict(myLRtrain, newdata=myBrokerDF, type = \"prob\")[,2] (myLRtrainCM <- table(myBrokerDF$Up_Down, myLRtrainpred)) ## myLRtrainpred ## Down Up ## Down 87 19 ## Up 18 64 1-sum(diag(myLRtrainCM))/sum(myLRtrainCM) ## [1] 0.1968085 # ROC for Logistic regression myLRPredict <- predict(myLRtrain, newdata=myBrokerDF, type=\"prob\") myLRPred <- prediction(myLRPredict[,2], myBrokerDF$Up_Down, label.ordering=c( \"Down\", \"Up\")) myLRPerf <- performance(myLRPred, \"tpr\", \"fpr\") performance(myLRPred, \"auc\") ## An object of class \"performance\" ## Slot \"x.name\": ## [1] \"None\" ## ## Slot \"y.name\": ## [1] \"Area under the ROC curve\" ## ## Slot \"alpha.name\": ## [1] \"none\" ## ## Slot \"x.values\": ## list() ## ## Slot \"y.values\": ## [[1]] ## [1] 0.9111827 ## ## ## Slot \"alpha.values\": ## list() The misclassification rate for the logistic regression model on the test set (myLRtraintest) is 24.32%. The accuracy rate for the model is 75.68%. The misclassification rate for the logistic regression model when validating the entire data set (myLRtrainpred) is 19.68%. The accuracy rate for the model is 80.32%. The confusion matrix shows that 64 observations were correctly classified as \u201cUp\u201d and 87 correctly classified as \u201cDown\u201d. The features that were found to be most important for prediction are average quote counts for 2015, average total insured value for 2015 and 2017, hit ratio for 2015. Random Forest To train the model using Random Forest algorithm, the method specified as \u201crf\u201d, the metric set to \u201cROC\u201d, and the number of trees set to 1500 (as overfitting is not a concern with Random Forest). The response column \u201cUp_Down\u201d needs to be set to \u201cfactor\u201d before proceeding. We can evaluate our model first before tuning (with 500 trees) and then with 1500 trees. # Random Forest with cross validation & tuning the no. of trees # (as overfitting is not a concern with Random Forest) no_trees <- 1500 # no. of trees myRFtune <- train(Up_Down ~ ., data=BrokTrainData18, method=\"rf\", metric=\"ROC\", ntree = no_trees, trControl=trainControl(classProbs=TRUE, summaryFunction=twoClassSummary)) plot(myRFtune) myRFtune$results ## mtry ROC Sens Spec ROCSD SensSD SpecSD ## 1 2 0.8114030 0.7398858 0.6991620 0.04372114 0.09167648 0.1392029 ## 2 13 0.8200906 0.7570010 0.7278308 0.04157764 0.08786922 0.1086326 ## 3 25 0.8185020 0.7615775 0.7062938 0.04014404 0.09534216 0.1127736 myRFtune$bestTune ## mtry ## 2 13 myRFtune$finalModel ## ## Call: ## randomForest(x = x, y = y, ntree = ..1, mtry = param$mtry) ## Type of random forest: classification ## Number of trees: 1500 ## No. of variables tried at each split: 13 ## ## OOB estimate of error rate: 26.49% ## Confusion matrix: ## Down Up class.error ## Down 67 18 0.2117647 ## Up 22 44 0.3333333 myRFtunedpredtest <- predict(myRFtune, newdata=BrokTestData18) (myRFtunedConfusion <- table(BrokTestData18$Up_Down, myRFtunedpredtest)) ## myRFtunedpredtest ## Down Up ## Down 16 5 ## Up 5 11 1-sum(diag(myRFtunedConfusion))/sum(myRFtunedConfusion) ## [1] 0.2702703 myRFtunedpred <- predict(myRFtune, newdata=myBrokerDF) (myRFtunedConfusion <- table(myBrokerDF$Up_Down, myRFtunedpred)) ## myRFtunedpred ## Down Up ## Down 101 5 ## Up 5 77 plot(myRFtune) 1-sum(diag(myRFtunedConfusion))/sum(myRFtunedConfusion) ## [1] 0.05319149 The misclassification rate for the random forest model when validating using the test set (myRFtunedpredtest) is 27.03%. The accuracy rate for the model was 72.97%. The misclassification rate for the random forest model when validating the entire data set (myLRtrainpred) is 5.31%. The accuracy rate for the model was 94.69%. The OOB estimate error rate for random forest, with default 500 as number of trees was 30.46%. The OOB estimate error rate for random forest, with default 1500 as number of trees was 27.81% myRFtunePredict <- predict(myRFtune, newdata=myBrokerDF, type=\"prob\") myRFtunePred <- prediction(myRFtunePredict[,2], myBrokerDF$Up_Down, label.ordering=c( \"Down\", \"Up\")) myRFtunePerf <- performance(myRFtunePred, \"tpr\", \"fpr\") # Plotting ROC curves plot(myRFtunePerf, col=1) plot(myLRPerf, col=2, add=TRUE) plot(myRparttunePerf, col=3, add=TRUE) legend(0.7, 0.6, c(\"Random Forest\", \"Log. Reg.\", \"Class. Tree\"), col=1:3, lwd=3) # Calculating AUC for all models performance(myRparttunePred, \"auc\") ## An object of class \"performance\" ## Slot \"x.name\": ## [1] \"None\" ## ## Slot \"y.name\": ## [1] \"Area under the ROC curve\" ## ## Slot \"alpha.name\": ## [1] \"none\" ## ## Slot \"x.values\": ## list() ## ## Slot \"y.values\": ## [[1]] ## [1] 0.8584906 ## ## ## Slot \"alpha.values\": ## list() performance(myLRPred, \"auc\") ## An object of class \"performance\" ## Slot \"x.name\": ## [1] \"None\" ## ## Slot \"y.name\": ## [1] \"Area under the ROC curve\" ## ## Slot \"alpha.name\": ## [1] \"none\" ## ## Slot \"x.values\": ## list() ## ## Slot \"y.values\": ## [[1]] ## [1] 0.9111827 ## ## ## Slot \"alpha.values\": ## list() performance(myRFtunePred, \"auc\") ## An object of class \"performance\" ## Slot \"x.name\": ## [1] \"None\" ## ## Slot \"y.name\": ## [1] \"Area under the ROC curve\" ## ## Slot \"alpha.name\": ## [1] \"none\" ## ## Slot \"x.values\": ## list() ## ## Slot \"y.values\": ## [[1]] ## [1] 0.9932121 ## ## ## Slot \"alpha.values\": ## list() Results: GWP 2019 Performance for the classification tree model, the logistic regression model and random forest model are represented by the green (Class Tree), red (Log Reg) and the black (Random Forest) curves. The AUC for the Classification Trees is 84.92, the AUC for the Logistic Regression is 91.12 and the AUC for Random Forest is 99.33 GWP 2019 Prediction For prediction of Gross Written Premium for 2019, we consider the two classification methods that yielded best results from the GWP 2018 prediction: classification trees and logistic regression. The variable previously used for the Gross Written Premium prediction for 2018: \u201cUp_Down\u201d, can be used for the prediction of GWP 2019. Next steps: First, we edit the data frame \u201cmyBroker_exp_DF\u201d to exclude all variables recorded for 2015. Second, all 2018 variables have to be included into the data frame (2016 to 2018) and third, the new data frame \u201cmyBrokerDF\u201d needs to be rerun to refresh the new variables. Further on we will follow all the steps that were previously done using the classification tree and logistic regression method to determine the GWP 2018 prediction, to predict the Gross Written Premium prediction for 2019. myBroker_exp_DF <- myBDF %>% dplyr::mutate(quote_ratio2016 = QuoteCount_2016/ Submissions_2016, quote_ratio2017 = QuoteCount_2017/ Submissions_2017, quote_ratio2018 = QuoteCount_2018/ Submissions_2018, hit_ratio16 = PolicyCount_2016/ QuoteCount_2016, hit_ratio17 = PolicyCount_2017/ QuoteCount_2017, hit_ratio18 = PolicyCount_2018/ QuoteCount_2018, success_ratio16_18 = PolicyCount_2016 + PolicyCount_2017 + PolicyCount_2018/ Submissions_2016 + Submissions_2017 + Submissions_2018, Up_Down) %>% select(-Submissions_2015, -QuoteCount_2015, -PolicyCount_2015, -AvgTIV_2015, -AvgQuote_2015, -GWP_2015, -Submissions_2014, -QuoteCount_2013, -QuoteCount_2014, -AvgQuote_2013, -AvgQuote_2014) myBroker_exp_DF$Up_Down <- as.factor(myBroker_exp_DF$Up_Down) myBrokerDF <- myBroker_exp_DF colnames(myBrokerDF) <- c(\"Submissions_3\", \"Submissions_2\", \"Submissions_1\", \"QuoteCount_3\",\"QuoteCount_2\", \"QuoteCount_1\", \"AvgQuote_3\", \"AvgQuote_2\", \"AvgQuote_1\", \"PolicyCount_3\", \"PolicyCount_2\", \"PolicyCount_1\", \"GWP_3\", \"GWP_2\", \"GWP_1\", \"AvgTIV_3\", \"AvgTIV_2\", \"AvgTIV1\", \"QR3\", \"QR2\", \"QR1\", \"HR3\", \"HR2\", \"HR1\",\"SR\",\"Up_Down\") trainRows <- createDataPartition(Up_Down, p = 0.8, list=FALSE) BrokTrainData <- myBrokerDF[trainRows,] BrokTestData <- myBrokerDF[-trainRows,] table(Up_Down) ## Up_Down ## Down Up ## 106 82 Classification Trees # Rpart # tuning & cross validation splitEntropy = list(split = c(\"information\")) myRparttune <- train(Up_Down ~ ., data=BrokTrainData, method=\"rpart\", metric=\"ROC\", tuneLength = 10, parms = splitEntropy, trControl=trainControl(classProbs=TRUE, summaryFunction=twoClassSummary)) plot(myRparttune) myRparttune$results ## cp ROC Sens Spec ROCSD SensSD SpecSD ## 1 0.00000000 0.7772342 0.7718559 0.7003976 0.05924325 0.1002510 0.1228265 ## 2 0.04713805 0.7781164 0.7738181 0.7072454 0.07481055 0.1036247 0.1352125 ## 3 0.09427609 0.7751503 0.7662576 0.6841993 0.07661726 0.1101780 0.1622165 ## 4 0.14141414 0.7543121 0.7264026 0.7051762 0.08868316 0.1541780 0.1738824 ## 5 0.18855219 0.7481983 0.6496310 0.7975082 0.07593846 0.1569490 0.1859312 ## 6 0.23569024 0.7377948 0.6062637 0.8506126 0.06647601 0.1184918 0.1663785 ## 7 0.28282828 0.7111638 0.6120016 0.8103259 0.08725597 0.1349080 0.2757272 ## 8 0.32996633 0.7013860 0.6280016 0.7747703 0.09657012 0.1555639 0.3190783 ## 9 0.37710438 0.6678546 0.6705642 0.6651450 0.11118447 0.1975369 0.4017962 ## 10 0.42424242 0.6265475 0.7388536 0.5142413 0.11769807 0.2243999 0.4455393 myRparttune$bestTune ## cp ## 2 0.04713805 myRparttune$finalModel ## n= 151 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 151 66 Down (0.56291391 0.43708609) ## 2) GWP_1< 1218364 53 3 Down (0.94339623 0.05660377) * ## 3) GWP_1>=1218364 98 35 Up (0.35714286 0.64285714) ## 6) AvgQuote_2< 30219.13 53 23 Down (0.56603774 0.43396226) ## 12) AvgTIV_2>=7232249 22 3 Down (0.86363636 0.13636364) * ## 13) AvgTIV_2< 7232249 31 11 Up (0.35483871 0.64516129) * ## 7) AvgQuote_2>=30219.13 45 5 Up (0.11111111 0.88888889) * print(myRparttune) ## CART ## ## 151 samples ## 25 predictor ## 2 classes: 'Down', 'Up' ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 151, 151, 151, 151, 151, 151, ... ## Resampling results across tuning parameters: ## ## cp ROC Sens Spec ## 0.00000000 0.7772342 0.7718559 0.7003976 ## 0.04713805 0.7781164 0.7738181 0.7072454 ## 0.09427609 0.7751503 0.7662576 0.6841993 ## 0.14141414 0.7543121 0.7264026 0.7051762 ## 0.18855219 0.7481983 0.6496310 0.7975082 ## 0.23569024 0.7377948 0.6062637 0.8506126 ## 0.28282828 0.7111638 0.6120016 0.8103259 ## 0.32996633 0.7013860 0.6280016 0.7747703 ## 0.37710438 0.6678546 0.6705642 0.6651450 ## 0.42424242 0.6265475 0.7388536 0.5142413 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.04713805. par(xpd = NA) plot(myRparttune$finalModel) text(myRparttune$finalModel, cex=.6) myRparttunepredtest <- predict(myRparttune, newdata=BrokTestData) (myRparttunedConfusion <- table(BrokTestData$Up_Down, myRparttunepredtest)) ## myRparttunepredtest ## Down Up ## Down 18 3 ## Up 3 13 1-sum(diag(myRparttunedConfusion))/sum(myRparttunedConfusion) ## [1] 0.1621622 myRparttunepredprob <- predict(myRparttune, newdata=myBrokerDF, type=\"prob\")[,2] myRparttunedpred <- predict(myRparttune, newdata=myBrokerDF) (myRparttunedCM <- table(myBrokerDF$Up_Down, myRparttunedpred)) ## myRparttunedpred ## Down Up ## Down 87 19 ## Up 9 73 1-sum(diag(myRparttunedCM))/sum(myRparttunedCM) ## [1] 0.1489362 # ROC myRparttunePredict <- predict(myRparttune, newdata=myBrokerDF, type=\"prob\") myRparttunePred <- prediction(myRparttunePredict[,2], myBrokerDF$Up_Down, label.ordering=c( \"Down\", \"Up\")) myRparttunePerf <- performance(myRparttunePred, \"tpr\", \"fpr\") performance(myRparttunePred, \"auc\") ## An object of class \"performance\" ## Slot \"x.name\": ## [1] \"None\" ## ## Slot \"y.name\": ## [1] \"Area under the ROC curve\" ## ## Slot \"alpha.name\": ## [1] \"none\" ## ## Slot \"x.values\": ## list() ## ## Slot \"y.values\": ## [[1]] ## [1] 0.890359 ## ## ## Slot \"alpha.values\": ## list() The misclassification rate for the classification tree model when validating the test set (myRparttunepredtest) is 18.92 %. The accuracy rate for the model was 81.08%. The misclassification rate for the classification tree model when validating entire data set (myRparttunedpred) is 15.43 %. The accuracy rate for this model was 84.57%. The features that appear to be most important for predicting the outcome for the gross written premium prediction for 2019 are Gross Written Premium, Average Quote, and Average Total Insured Value. Logistic Regression # Logistic regression # Cross Validation myLRtrain <- train(Up_Down ~ ., data=BrokTrainData, method=\"glm\", metric=\"ROC\", tuneLength = 10, trControl=trainControl(classProbs=TRUE, summaryFunction=twoClassSummary)) myLRtrain$results ## parameter ROC Sens Spec ROCSD SensSD SpecSD ## 1 none 0.8936167 0.8720923 0.8428428 0.05939975 0.0736007 0.07988117 myLRtrain$bestTune ## parameter ## 1 none myLRtrain$finalModel ## ## Call: NULL ## ## Coefficients: ## (Intercept) Submissions_3 Submissions_2 Submissions_1 QuoteCount_3 ## -1.150e+00 3.181e-02 2.365e+01 2.356e+01 -1.107e-01 ## QuoteCount_2 QuoteCount_1 AvgQuote_3 AvgQuote_2 AvgQuote_1 ## -1.723e-01 -5.991e-02 -1.062e-03 1.214e-03 -8.091e-04 ## PolicyCount_3 PolicyCount_2 PolicyCount_1 GWP_3 GWP_2 ## 2.313e+01 2.278e+01 7.174e-01 8.688e-06 -3.028e-04 ## GWP_1 AvgTIV_3 AvgTIV_2 AvgTIV1 QR3 ## 3.162e-04 -1.397e-05 -1.837e-06 1.062e-05 2.361e+01 ## QR2 QR1 HR3 HR2 HR1 ## 1.494e+01 2.155e+01 1.019e+00 1.949e+00 2.520e+00 ## SR ## -2.348e+01 ## ## Degrees of Freedom: 150 Total (i.e. Null); 125 Residual ## Null Deviance: 206.9 ## Residual Deviance: 3.366e-08 AIC: 52 print(myLRtrain) ## Generalized Linear Model ## ## 151 samples ## 25 predictor ## 2 classes: 'Down', 'Up' ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 151, 151, 151, 151, 151, 151, ... ## Resampling results: ## ## ROC Sens Spec ## 0.8936167 0.8720923 0.8428428 myLRtraintest <- predict(myLRtrain, newdata=BrokTestData) (myLRtrainConfusion <- table(BrokTestData$Up_Down, myLRtraintest)) ## myLRtraintest ## Down Up ## Down 20 1 ## Up 4 12 1-sum(diag(myLRtrainConfusion))/sum(myLRtrainConfusion) ## [1] 0.1351351 myLRtrainpred <- predict(myLRtrain, newdata=myBrokerDF) (myLRtrainCM <- table(myBrokerDF$Up_Down, myLRtrainpred)) ## myLRtrainpred ## Down Up ## Down 105 1 ## Up 4 78 1-sum(diag(myLRtrainCM))/sum(myLRtrainCM) ## [1] 0.02659574 # ROC myLRPredict <- predict(myLRtrain, newdata=myBrokerDF, type=\"prob\") myLRPred <- prediction(myLRPredict[,2], myBrokerDF$Up_Down, label.ordering=c( \"Down\", \"Up\")) myLRPerf <- performance(myLRPred, \"tpr\", \"fpr\") performance(myLRPred, \"auc\") ## An object of class \"performance\" ## Slot \"x.name\": ## [1] \"None\" ## ## Slot \"y.name\": ## [1] \"Area under the ROC curve\" ## ## Slot \"alpha.name\": ## [1] \"none\" ## ## Slot \"x.values\": ## list() ## ## Slot \"y.values\": ## [[1]] ## [1] 0.9983893 ## ## ## Slot \"alpha.values\": ## list() The misclassification rate for the logistic regression model when validating the test set (myLRtraintest) is 13.51%. The accuracy rate for the model was 86.49%. The misclassification rate for the logistic regression model when validating the entire data set (myLRtrainpred) is 2.65%. The accuracy rate for the model was 97.35%. plot(myLRPerf, col=1) plot(myRparttunePerf, col=2, add=TRUE) legend(0.7, 0.6, c(\"Log. Reg.\", \"Class. Tree\"), col=1:2, lwd=10) Results: GWP 2019 The performance for the logistic regression model and the classification tree model are represented by the red (Class Tree) and black (Log Reg) curves. The AUC for the Class Tree is 87.51 and the AUC for the Log Reg is 99.11. Conclusion To conclude with regards to the Broker Segmentation, task 1: the hierarchical and k-means clustering methods were performed and analyzed. Of the two methods, we preferred the k-means clustering approach. Although both methods performed well during evaluation, the K-means clustering approach appeared to perform slightly better and provided better results. The PCA scores and average silhouette coefficients obtained from this method presented better assignment of clusters overall. With regards to Gross Written Premium Prediction, task 2: In comparison to the classification tree and logistic regression, the random forest method, appeared to perform better in the ROC curves. The accuracy rate was 94.69% and misclassification rate 5.32%. This method also provided the highest area under the curve (AUC) = 99%, and high rates for specificity (95%) and sensitivity (94%). While this method seems to be the highest performer, we considered that the results were unrealistic or \u201ctoo good to be true\u201d and high performance could be attributed to data leakage or noise. At times when a complex model, like random forest is used, in a small data set such as the one utilized in this case, the function does not have enough information to train on. Hence, the the best approach for GWP 2018 prediction is the Logistic regression model. The predicted outcomes appeared more meaningful and useful for consideration, and the model seemed to provide more realistic results. Performance results obtained from the model were satisfactory with low misclassification rate of 19.68% and elevated accuracy rate of 80.32%. The AUC was 91.12%, sensitivity 78% and specificity 82%. The features most important for prediction were average quote counts for 2015, average total insured value for 2015 and 2017, hit ratio for 2015. To predict whether the gross premium will increase or decrease for 2019: the logistic regression model appeared to perform better in the ROC curve. The accuracy rate for this model was 97.35% and the misclassification rate was 2.65%. The AUC for this model was 99%, specificity was 98%. and sensitivity was 96.34 %. Again, due to unrealistic performance results, we would prefer the other method. Our recommendation for 2019 prediction has to be the classification tree model as it also provided high performance values. The AUC for this model was 87.51%, specificity was 84% and sensitivity was 85.3%. The misclassification rate obtained was 15.43% and the accuracy rate for this model was 84.57%. The most important features for prediction appear to be GWP 2016, 2017, and 2018, policy counts 2016, 2017, and 2018, as well as the avg. quote for 2017 and 2018, and avg. TIV for 2017 and 2018.","title":"Modelling"},{"location":"Prediction/#gwp-2018-prediction","text":"For prediction of Gross Written Premium for 2019 we need to first predict for 2018 to test our model accuracy. Since we are primarily interested in determing whether GWP increases or decreases for the upcoming year for each broker, a new column called \"Up_Down\" can be created that compares GWP 2018 values to GWP 2017 and labels the values as \"Up\" if GWP for 2018 exceeds 2017 and \"Down\" if GWP for 2018 is lower than 2017. The response variable is a factor (character variable), we have to use machine learning algorithms such as classification trees, logistic regression, etc. The new dataframe: \u201cmyBroker_exp_DF\u201d consists of all variables from 2015 to 2017. Variables from years 2013 and 2014 are excluded, and the new variable: \"Up_Down\" is added. # Prediction of GWP2018 set.seed(12345) Up_Down <- character(length(myBDF$GWP_2018)) Up_Down[myBDF$GWP_2017 < myBDF$GWP_2018] <- \"Up\" Up_Down[myBDF$GWP_2017 >= myBDF$GWP_2018] <- \"Down\" myBroker_exp_DF <- myBDF %>% dplyr::mutate(quote_ratio2015 = QuoteCount_2015/ Submissions_2015, quote_ratio2016 = QuoteCount_2016/ Submissions_2016, quote_ratio2017 = QuoteCount_2017/ Submissions_2017, hit_ratio15 = PolicyCount_2015/ QuoteCount_2015, hit_ratio16 = PolicyCount_2016/ QuoteCount_2016, hit_ratio17 = PolicyCount_2017/ QuoteCount_2017, success_ratio15_17 = PolicyCount_2015+ PolicyCount_2016 + PolicyCount_2017/ Submissions_2015 + Submissions_2016 + Submissions_2017, Up_Down) %>% select(-GWP_2018, -Submissions_2018, -QuoteCount_2018, -PolicyCount_2018, -AvgQuote_2018, -AvgTIV_2018, -Submissions_2014, -QuoteCount_2013, -QuoteCount_2014, -AvgQuote_2013, -AvgQuote_2014) myBroker_exp_DF$Up_Down <- as.factor(myBroker_exp_DF$Up_Down) myBrokerDF <- myBroker_exp_DF colnames(myBrokerDF) <- c(\"Submissions_3\", \"Submissions_2\", \"Submissions_1\", \"QuoteCount_3\",\"QuoteCount_2\", \"QuoteCount_1\", \"AvgQuote_3\", \"AvgQuote_2\", \"AvgQuote_1\", \"PolicyCount_3\", \"PolicyCount_2\", \"PolicyCount_1\", \"GWP_3\", \"GWP_2\", \"GWP_1\", \"AvgTIV_3\", \"AvgTIV_2\", \"AvgTIV1\", \"QR3\", \"QR2\", \"QR1\", \"HR3\", \"HR2\", \"HR1\",\"SR\",\"Up_Down\") We then proceed to partition the dataset into test (20%) and training (80%) using the function \u201ccreateDataPartition\u201d (found in the \"caret\" package). # Partitioning into training and test, training = 80% as the dataset is small trainRows <- createDataPartition(Up_Down, p = 0.8, list=FALSE) BrokTrainData18 <- myBrokerDF[trainRows,] BrokTestData18 <- myBrokerDF[-trainRows,] table(Up_Down) ## Up_Down ## Down Up ## 106 82","title":"GWP 2018 Prediction"},{"location":"Prediction/#classification-trees","text":"For the classification tree model, we use all variables in the training set to train the model. To train the model, we use the train function and save the results in \"\u201cmyRparttune\u201d, apply the \u201crpart\u201d method, and set the metric to \u201cROC\u201d to ensure the selection of best model. For tuning the model we need to include; tunelength = 10, default being 3, and split criteria set to \u201cinformation gain\u201d, default is Gini Index. The train function provided the best model at complexity parameter = 0.07. # Rpart # increasing tune length & split type as 'Entropy' splitEntropy = list(split = c(\"information\")) myRparttune <- train(Up_Down ~ ., data=BrokTrainData18, method=\"rpart\", metric=\"ROC\", tuneLength = 10, parms = splitEntropy, trControl=trainControl(classProbs=TRUE, summaryFunction=twoClassSummary)) plot(myRparttune) myRparttune$results ## cp ROC Sens Spec ROCSD SensSD SpecSD ## 1 0.00000000 0.7087193 0.7030466 0.6596959 0.06744125 0.1134558 0.1158620 ## 2 0.03535354 0.7025369 0.7053459 0.6661077 0.07400825 0.1096479 0.1015546 ## 3 0.07070707 0.6943522 0.7022634 0.6420807 0.07094654 0.1119068 0.1225912 ## 4 0.10606061 0.6882251 0.6545418 0.6915282 0.07224751 0.1549664 0.1601590 ## 5 0.14141414 0.6809741 0.6552857 0.6663226 0.08001456 0.1753808 0.1887295 ## 6 0.17676768 0.6757820 0.6695795 0.6547226 0.07962344 0.1698370 0.2027911 ## 7 0.21212121 0.6703244 0.6802461 0.6307015 0.08585172 0.1801601 0.2446506 ## 8 0.24747475 0.6499109 0.6265344 0.6665035 0.08323140 0.1763311 0.2818400 ## 9 0.28282828 0.6260870 0.6584177 0.5937563 0.09149758 0.1980458 0.3535952 ## 10 0.31818182 0.6106691 0.6954677 0.5258706 0.09481431 0.2145107 0.3797248 myRparttune$bestTune # best model with cp ## cp ## 1 0 myRparttune$finalModel ## n= 151 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 151 66 Down (0.56291391 0.43708609) ## 2) PolicyCount_1< 45.5 52 6 Down (0.88461538 0.11538462) * ## 3) PolicyCount_1>=45.5 99 39 Up (0.39393939 0.60606061) ## 6) AvgQuote_1< 30219.13 58 23 Down (0.60344828 0.39655172) ## 12) AvgTIV1>=7156371 29 5 Down (0.82758621 0.17241379) * ## 13) AvgTIV1< 7156371 29 11 Up (0.37931034 0.62068966) ## 26) HR2>=0.7449327 10 3 Down (0.70000000 0.30000000) * ## 27) HR2< 0.7449327 19 4 Up (0.21052632 0.78947368) * ## 7) AvgQuote_1>=30219.13 41 4 Up (0.09756098 0.90243902) * print(myRparttune) ## CART ## ## 151 samples ## 25 predictor ## 2 classes: 'Down', 'Up' ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 151, 151, 151, 151, 151, 151, ... ## Resampling results across tuning parameters: ## ## cp ROC Sens Spec ## 0.00000000 0.7087193 0.7030466 0.6596959 ## 0.03535354 0.7025369 0.7053459 0.6661077 ## 0.07070707 0.6943522 0.7022634 0.6420807 ## 0.10606061 0.6882251 0.6545418 0.6915282 ## 0.14141414 0.6809741 0.6552857 0.6663226 ## 0.17676768 0.6757820 0.6695795 0.6547226 ## 0.21212121 0.6703244 0.6802461 0.6307015 ## 0.24747475 0.6499109 0.6265344 0.6665035 ## 0.28282828 0.6260870 0.6584177 0.5937563 ## 0.31818182 0.6106691 0.6954677 0.5258706 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0. par(xpd = NA) plot(myRparttune$finalModel) text(myRparttune$finalModel, cex=.6) myRparttunepredtest <- predict(myRparttune, newdata=BrokTestData18) (myRparttunedConfusion <- table(BrokTestData18$Up_Down, myRparttunepredtest)) ## myRparttunepredtest ## Down Up ## Down 17 4 ## Up 3 13 1-sum(diag(myRparttunedConfusion))/sum(myRparttunedConfusion) ## [1] 0.1891892 myRparttunedpred <- predict(myRparttune, newdata=myBrokerDF) (myRparttunedCM <- table(myBrokerDF$Up_Down, myRparttunedpred)) ## myRparttunedpred ## Down Up ## Down 94 12 ## Up 17 65 1-sum(diag(myRparttunedCM))/sum(myRparttunedCM) ## [1] 0.1542553 myRparttunePredict <- predict(myRparttune, newdata=myBrokerDF, type=\"prob\") myRparttunePred <- prediction(myRparttunePredict[,2], myBrokerDF$Up_Down, label.ordering=c( \"Down\", \"Up\")) myRparttunePerf <- performance(myRparttunePred, \"tpr\", \"fpr\") performance(myRparttunePred, \"auc\") ## An object of class \"performance\" ## Slot \"x.name\": ## [1] \"None\" ## ## Slot \"y.name\": ## [1] \"Area under the ROC curve\" ## ## Slot \"alpha.name\": ## [1] \"none\" ## ## Slot \"x.values\": ## list() ## ## Slot \"y.values\": ## [[1]] ## [1] 0.8584906 ## ## ## Slot \"alpha.values\": ## list() The misclassification rate for the classification tree model when validating on the test data set (myRparttunepredtest) was 24.32 %. The accuracy rate for the model was 75.68%. The matrix represents a total of 13 observations correctly classified as \u201cUp\u201d and 15 observations correctly classified as \u201cDown\u201d. The misclassification rate for classification tree model when validated using the entire data set (myRparttunedpred) is 18.62 %. The accuracy rate for this model is 81.38%. The matrix shows a total of 68 observations correctly classified as \u201cUp\u201d and 85 observations correctly classified as \u201cDown\u201d. The variables that appear to be important for predicting the outcome for the gross written premium prediction for 2018 are Policy Counts, Average Quote, and Average Total Insured Value.","title":"Classification Trees"},{"location":"Prediction/#logistic-regression","text":"The logistic regression model using the training dataset (BrokTrainData18): Weights are not assigned as there is \u201clittle\u201d to \u201cno\u201d class imbalance (Up: 82, Dow: 106). To train the model, we use the train function (and store results in \u201cmyLRtrain\u201d), apply the \u201cglm\u201d method and set the metric to \u201cROC\u201d. # Logistic regression #The tuneLength parameter is used to determine the total number of combinations that will be evaluated myLRtrain <- train(Up_Down ~ ., data=BrokTrainData18, method=\"glm\", metric=\"ROC\", tuneLength = 10, trControl=trainControl(classProbs=TRUE, summaryFunction=twoClassSummary)) myLRtrain$results ## parameter ROC Sens Spec ROCSD SensSD SpecSD ## 1 none 0.7701864 0.7088419 0.6758822 0.06620168 0.09827614 0.1016044 myLRtrain$bestTune ## parameter ## 1 none myLRtrain$finalModel ## ## Call: NULL ## ## Coefficients: ## (Intercept) Submissions_3 Submissions_2 Submissions_1 QuoteCount_3 ## 9.596e-01 -5.909e-03 -3.232e+00 -3.234e+00 8.027e-03 ## QuoteCount_2 QuoteCount_1 AvgQuote_3 AvgQuote_2 AvgQuote_1 ## -1.148e-02 -5.399e-03 3.276e-05 -1.346e-05 2.173e-04 ## PolicyCount_3 PolicyCount_2 PolicyCount_1 GWP_3 GWP_2 ## -3.280e+00 -3.243e+00 7.454e-02 -6.097e-07 -6.275e-07 ## GWP_1 AvgTIV_3 AvgTIV_2 AvgTIV1 QR3 ## 4.961e-07 8.505e-07 -1.898e-07 -1.563e-06 -4.295e+00 ## QR2 QR1 HR3 HR2 HR1 ## 1.361e+00 2.944e-01 -7.731e-01 -1.279e-01 -1.896e+00 ## SR ## 3.239e+00 ## ## Degrees of Freedom: 150 Total (i.e. Null); 125 Residual ## Null Deviance: 206.9 ## Residual Deviance: 95.49 AIC: 147.5 print(myLRtrain) ## Generalized Linear Model ## ## 151 samples ## 25 predictor ## 2 classes: 'Down', 'Up' ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 151, 151, 151, 151, 151, 151, ... ## Resampling results: ## ## ROC Sens Spec ## 0.7701864 0.7088419 0.6758822 myLRtraintest <- predict(myLRtrain, newdata=BrokTestData18) (myLRtrainConfusion <- table(BrokTestData18$Up_Down, myLRtraintest)) ## myLRtraintest ## Down Up ## Down 15 6 ## Up 3 13 1-sum(diag(myLRtrainConfusion))/sum(myLRtrainConfusion) ## [1] 0.2432432 myLRtrainpred <- predict(myLRtrain, newdata=myBrokerDF) myLRtrainprob <- predict(myLRtrain, newdata=myBrokerDF, type = \"prob\")[,2] (myLRtrainCM <- table(myBrokerDF$Up_Down, myLRtrainpred)) ## myLRtrainpred ## Down Up ## Down 87 19 ## Up 18 64 1-sum(diag(myLRtrainCM))/sum(myLRtrainCM) ## [1] 0.1968085 # ROC for Logistic regression myLRPredict <- predict(myLRtrain, newdata=myBrokerDF, type=\"prob\") myLRPred <- prediction(myLRPredict[,2], myBrokerDF$Up_Down, label.ordering=c( \"Down\", \"Up\")) myLRPerf <- performance(myLRPred, \"tpr\", \"fpr\") performance(myLRPred, \"auc\") ## An object of class \"performance\" ## Slot \"x.name\": ## [1] \"None\" ## ## Slot \"y.name\": ## [1] \"Area under the ROC curve\" ## ## Slot \"alpha.name\": ## [1] \"none\" ## ## Slot \"x.values\": ## list() ## ## Slot \"y.values\": ## [[1]] ## [1] 0.9111827 ## ## ## Slot \"alpha.values\": ## list() The misclassification rate for the logistic regression model on the test set (myLRtraintest) is 24.32%. The accuracy rate for the model is 75.68%. The misclassification rate for the logistic regression model when validating the entire data set (myLRtrainpred) is 19.68%. The accuracy rate for the model is 80.32%. The confusion matrix shows that 64 observations were correctly classified as \u201cUp\u201d and 87 correctly classified as \u201cDown\u201d. The features that were found to be most important for prediction are average quote counts for 2015, average total insured value for 2015 and 2017, hit ratio for 2015.","title":"Logistic Regression"},{"location":"Prediction/#random-forest","text":"To train the model using Random Forest algorithm, the method specified as \u201crf\u201d, the metric set to \u201cROC\u201d, and the number of trees set to 1500 (as overfitting is not a concern with Random Forest). The response column \u201cUp_Down\u201d needs to be set to \u201cfactor\u201d before proceeding. We can evaluate our model first before tuning (with 500 trees) and then with 1500 trees. # Random Forest with cross validation & tuning the no. of trees # (as overfitting is not a concern with Random Forest) no_trees <- 1500 # no. of trees myRFtune <- train(Up_Down ~ ., data=BrokTrainData18, method=\"rf\", metric=\"ROC\", ntree = no_trees, trControl=trainControl(classProbs=TRUE, summaryFunction=twoClassSummary)) plot(myRFtune) myRFtune$results ## mtry ROC Sens Spec ROCSD SensSD SpecSD ## 1 2 0.8114030 0.7398858 0.6991620 0.04372114 0.09167648 0.1392029 ## 2 13 0.8200906 0.7570010 0.7278308 0.04157764 0.08786922 0.1086326 ## 3 25 0.8185020 0.7615775 0.7062938 0.04014404 0.09534216 0.1127736 myRFtune$bestTune ## mtry ## 2 13 myRFtune$finalModel ## ## Call: ## randomForest(x = x, y = y, ntree = ..1, mtry = param$mtry) ## Type of random forest: classification ## Number of trees: 1500 ## No. of variables tried at each split: 13 ## ## OOB estimate of error rate: 26.49% ## Confusion matrix: ## Down Up class.error ## Down 67 18 0.2117647 ## Up 22 44 0.3333333 myRFtunedpredtest <- predict(myRFtune, newdata=BrokTestData18) (myRFtunedConfusion <- table(BrokTestData18$Up_Down, myRFtunedpredtest)) ## myRFtunedpredtest ## Down Up ## Down 16 5 ## Up 5 11 1-sum(diag(myRFtunedConfusion))/sum(myRFtunedConfusion) ## [1] 0.2702703 myRFtunedpred <- predict(myRFtune, newdata=myBrokerDF) (myRFtunedConfusion <- table(myBrokerDF$Up_Down, myRFtunedpred)) ## myRFtunedpred ## Down Up ## Down 101 5 ## Up 5 77 plot(myRFtune) 1-sum(diag(myRFtunedConfusion))/sum(myRFtunedConfusion) ## [1] 0.05319149 The misclassification rate for the random forest model when validating using the test set (myRFtunedpredtest) is 27.03%. The accuracy rate for the model was 72.97%. The misclassification rate for the random forest model when validating the entire data set (myLRtrainpred) is 5.31%. The accuracy rate for the model was 94.69%. The OOB estimate error rate for random forest, with default 500 as number of trees was 30.46%. The OOB estimate error rate for random forest, with default 1500 as number of trees was 27.81% myRFtunePredict <- predict(myRFtune, newdata=myBrokerDF, type=\"prob\") myRFtunePred <- prediction(myRFtunePredict[,2], myBrokerDF$Up_Down, label.ordering=c( \"Down\", \"Up\")) myRFtunePerf <- performance(myRFtunePred, \"tpr\", \"fpr\") # Plotting ROC curves plot(myRFtunePerf, col=1) plot(myLRPerf, col=2, add=TRUE) plot(myRparttunePerf, col=3, add=TRUE) legend(0.7, 0.6, c(\"Random Forest\", \"Log. Reg.\", \"Class. Tree\"), col=1:3, lwd=3) # Calculating AUC for all models performance(myRparttunePred, \"auc\") ## An object of class \"performance\" ## Slot \"x.name\": ## [1] \"None\" ## ## Slot \"y.name\": ## [1] \"Area under the ROC curve\" ## ## Slot \"alpha.name\": ## [1] \"none\" ## ## Slot \"x.values\": ## list() ## ## Slot \"y.values\": ## [[1]] ## [1] 0.8584906 ## ## ## Slot \"alpha.values\": ## list() performance(myLRPred, \"auc\") ## An object of class \"performance\" ## Slot \"x.name\": ## [1] \"None\" ## ## Slot \"y.name\": ## [1] \"Area under the ROC curve\" ## ## Slot \"alpha.name\": ## [1] \"none\" ## ## Slot \"x.values\": ## list() ## ## Slot \"y.values\": ## [[1]] ## [1] 0.9111827 ## ## ## Slot \"alpha.values\": ## list() performance(myRFtunePred, \"auc\") ## An object of class \"performance\" ## Slot \"x.name\": ## [1] \"None\" ## ## Slot \"y.name\": ## [1] \"Area under the ROC curve\" ## ## Slot \"alpha.name\": ## [1] \"none\" ## ## Slot \"x.values\": ## list() ## ## Slot \"y.values\": ## [[1]] ## [1] 0.9932121 ## ## ## Slot \"alpha.values\": ## list()","title":"Random Forest"},{"location":"Prediction/#results-gwp-2019","text":"Performance for the classification tree model, the logistic regression model and random forest model are represented by the green (Class Tree), red (Log Reg) and the black (Random Forest) curves. The AUC for the Classification Trees is 84.92, the AUC for the Logistic Regression is 91.12 and the AUC for Random Forest is 99.33","title":"Results: GWP 2019"},{"location":"Prediction/#gwp-2019-prediction","text":"For prediction of Gross Written Premium for 2019, we consider the two classification methods that yielded best results from the GWP 2018 prediction: classification trees and logistic regression. The variable previously used for the Gross Written Premium prediction for 2018: \u201cUp_Down\u201d, can be used for the prediction of GWP 2019. Next steps: First, we edit the data frame \u201cmyBroker_exp_DF\u201d to exclude all variables recorded for 2015. Second, all 2018 variables have to be included into the data frame (2016 to 2018) and third, the new data frame \u201cmyBrokerDF\u201d needs to be rerun to refresh the new variables. Further on we will follow all the steps that were previously done using the classification tree and logistic regression method to determine the GWP 2018 prediction, to predict the Gross Written Premium prediction for 2019. myBroker_exp_DF <- myBDF %>% dplyr::mutate(quote_ratio2016 = QuoteCount_2016/ Submissions_2016, quote_ratio2017 = QuoteCount_2017/ Submissions_2017, quote_ratio2018 = QuoteCount_2018/ Submissions_2018, hit_ratio16 = PolicyCount_2016/ QuoteCount_2016, hit_ratio17 = PolicyCount_2017/ QuoteCount_2017, hit_ratio18 = PolicyCount_2018/ QuoteCount_2018, success_ratio16_18 = PolicyCount_2016 + PolicyCount_2017 + PolicyCount_2018/ Submissions_2016 + Submissions_2017 + Submissions_2018, Up_Down) %>% select(-Submissions_2015, -QuoteCount_2015, -PolicyCount_2015, -AvgTIV_2015, -AvgQuote_2015, -GWP_2015, -Submissions_2014, -QuoteCount_2013, -QuoteCount_2014, -AvgQuote_2013, -AvgQuote_2014) myBroker_exp_DF$Up_Down <- as.factor(myBroker_exp_DF$Up_Down) myBrokerDF <- myBroker_exp_DF colnames(myBrokerDF) <- c(\"Submissions_3\", \"Submissions_2\", \"Submissions_1\", \"QuoteCount_3\",\"QuoteCount_2\", \"QuoteCount_1\", \"AvgQuote_3\", \"AvgQuote_2\", \"AvgQuote_1\", \"PolicyCount_3\", \"PolicyCount_2\", \"PolicyCount_1\", \"GWP_3\", \"GWP_2\", \"GWP_1\", \"AvgTIV_3\", \"AvgTIV_2\", \"AvgTIV1\", \"QR3\", \"QR2\", \"QR1\", \"HR3\", \"HR2\", \"HR1\",\"SR\",\"Up_Down\") trainRows <- createDataPartition(Up_Down, p = 0.8, list=FALSE) BrokTrainData <- myBrokerDF[trainRows,] BrokTestData <- myBrokerDF[-trainRows,] table(Up_Down) ## Up_Down ## Down Up ## 106 82","title":"GWP 2019 Prediction"},{"location":"Prediction/#classification-trees_1","text":"# Rpart # tuning & cross validation splitEntropy = list(split = c(\"information\")) myRparttune <- train(Up_Down ~ ., data=BrokTrainData, method=\"rpart\", metric=\"ROC\", tuneLength = 10, parms = splitEntropy, trControl=trainControl(classProbs=TRUE, summaryFunction=twoClassSummary)) plot(myRparttune) myRparttune$results ## cp ROC Sens Spec ROCSD SensSD SpecSD ## 1 0.00000000 0.7772342 0.7718559 0.7003976 0.05924325 0.1002510 0.1228265 ## 2 0.04713805 0.7781164 0.7738181 0.7072454 0.07481055 0.1036247 0.1352125 ## 3 0.09427609 0.7751503 0.7662576 0.6841993 0.07661726 0.1101780 0.1622165 ## 4 0.14141414 0.7543121 0.7264026 0.7051762 0.08868316 0.1541780 0.1738824 ## 5 0.18855219 0.7481983 0.6496310 0.7975082 0.07593846 0.1569490 0.1859312 ## 6 0.23569024 0.7377948 0.6062637 0.8506126 0.06647601 0.1184918 0.1663785 ## 7 0.28282828 0.7111638 0.6120016 0.8103259 0.08725597 0.1349080 0.2757272 ## 8 0.32996633 0.7013860 0.6280016 0.7747703 0.09657012 0.1555639 0.3190783 ## 9 0.37710438 0.6678546 0.6705642 0.6651450 0.11118447 0.1975369 0.4017962 ## 10 0.42424242 0.6265475 0.7388536 0.5142413 0.11769807 0.2243999 0.4455393 myRparttune$bestTune ## cp ## 2 0.04713805 myRparttune$finalModel ## n= 151 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 151 66 Down (0.56291391 0.43708609) ## 2) GWP_1< 1218364 53 3 Down (0.94339623 0.05660377) * ## 3) GWP_1>=1218364 98 35 Up (0.35714286 0.64285714) ## 6) AvgQuote_2< 30219.13 53 23 Down (0.56603774 0.43396226) ## 12) AvgTIV_2>=7232249 22 3 Down (0.86363636 0.13636364) * ## 13) AvgTIV_2< 7232249 31 11 Up (0.35483871 0.64516129) * ## 7) AvgQuote_2>=30219.13 45 5 Up (0.11111111 0.88888889) * print(myRparttune) ## CART ## ## 151 samples ## 25 predictor ## 2 classes: 'Down', 'Up' ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 151, 151, 151, 151, 151, 151, ... ## Resampling results across tuning parameters: ## ## cp ROC Sens Spec ## 0.00000000 0.7772342 0.7718559 0.7003976 ## 0.04713805 0.7781164 0.7738181 0.7072454 ## 0.09427609 0.7751503 0.7662576 0.6841993 ## 0.14141414 0.7543121 0.7264026 0.7051762 ## 0.18855219 0.7481983 0.6496310 0.7975082 ## 0.23569024 0.7377948 0.6062637 0.8506126 ## 0.28282828 0.7111638 0.6120016 0.8103259 ## 0.32996633 0.7013860 0.6280016 0.7747703 ## 0.37710438 0.6678546 0.6705642 0.6651450 ## 0.42424242 0.6265475 0.7388536 0.5142413 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.04713805. par(xpd = NA) plot(myRparttune$finalModel) text(myRparttune$finalModel, cex=.6) myRparttunepredtest <- predict(myRparttune, newdata=BrokTestData) (myRparttunedConfusion <- table(BrokTestData$Up_Down, myRparttunepredtest)) ## myRparttunepredtest ## Down Up ## Down 18 3 ## Up 3 13 1-sum(diag(myRparttunedConfusion))/sum(myRparttunedConfusion) ## [1] 0.1621622 myRparttunepredprob <- predict(myRparttune, newdata=myBrokerDF, type=\"prob\")[,2] myRparttunedpred <- predict(myRparttune, newdata=myBrokerDF) (myRparttunedCM <- table(myBrokerDF$Up_Down, myRparttunedpred)) ## myRparttunedpred ## Down Up ## Down 87 19 ## Up 9 73 1-sum(diag(myRparttunedCM))/sum(myRparttunedCM) ## [1] 0.1489362 # ROC myRparttunePredict <- predict(myRparttune, newdata=myBrokerDF, type=\"prob\") myRparttunePred <- prediction(myRparttunePredict[,2], myBrokerDF$Up_Down, label.ordering=c( \"Down\", \"Up\")) myRparttunePerf <- performance(myRparttunePred, \"tpr\", \"fpr\") performance(myRparttunePred, \"auc\") ## An object of class \"performance\" ## Slot \"x.name\": ## [1] \"None\" ## ## Slot \"y.name\": ## [1] \"Area under the ROC curve\" ## ## Slot \"alpha.name\": ## [1] \"none\" ## ## Slot \"x.values\": ## list() ## ## Slot \"y.values\": ## [[1]] ## [1] 0.890359 ## ## ## Slot \"alpha.values\": ## list() The misclassification rate for the classification tree model when validating the test set (myRparttunepredtest) is 18.92 %. The accuracy rate for the model was 81.08%. The misclassification rate for the classification tree model when validating entire data set (myRparttunedpred) is 15.43 %. The accuracy rate for this model was 84.57%. The features that appear to be most important for predicting the outcome for the gross written premium prediction for 2019 are Gross Written Premium, Average Quote, and Average Total Insured Value.","title":"Classification Trees"},{"location":"Prediction/#logistic-regression_1","text":"# Logistic regression # Cross Validation myLRtrain <- train(Up_Down ~ ., data=BrokTrainData, method=\"glm\", metric=\"ROC\", tuneLength = 10, trControl=trainControl(classProbs=TRUE, summaryFunction=twoClassSummary)) myLRtrain$results ## parameter ROC Sens Spec ROCSD SensSD SpecSD ## 1 none 0.8936167 0.8720923 0.8428428 0.05939975 0.0736007 0.07988117 myLRtrain$bestTune ## parameter ## 1 none myLRtrain$finalModel ## ## Call: NULL ## ## Coefficients: ## (Intercept) Submissions_3 Submissions_2 Submissions_1 QuoteCount_3 ## -1.150e+00 3.181e-02 2.365e+01 2.356e+01 -1.107e-01 ## QuoteCount_2 QuoteCount_1 AvgQuote_3 AvgQuote_2 AvgQuote_1 ## -1.723e-01 -5.991e-02 -1.062e-03 1.214e-03 -8.091e-04 ## PolicyCount_3 PolicyCount_2 PolicyCount_1 GWP_3 GWP_2 ## 2.313e+01 2.278e+01 7.174e-01 8.688e-06 -3.028e-04 ## GWP_1 AvgTIV_3 AvgTIV_2 AvgTIV1 QR3 ## 3.162e-04 -1.397e-05 -1.837e-06 1.062e-05 2.361e+01 ## QR2 QR1 HR3 HR2 HR1 ## 1.494e+01 2.155e+01 1.019e+00 1.949e+00 2.520e+00 ## SR ## -2.348e+01 ## ## Degrees of Freedom: 150 Total (i.e. Null); 125 Residual ## Null Deviance: 206.9 ## Residual Deviance: 3.366e-08 AIC: 52 print(myLRtrain) ## Generalized Linear Model ## ## 151 samples ## 25 predictor ## 2 classes: 'Down', 'Up' ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 151, 151, 151, 151, 151, 151, ... ## Resampling results: ## ## ROC Sens Spec ## 0.8936167 0.8720923 0.8428428 myLRtraintest <- predict(myLRtrain, newdata=BrokTestData) (myLRtrainConfusion <- table(BrokTestData$Up_Down, myLRtraintest)) ## myLRtraintest ## Down Up ## Down 20 1 ## Up 4 12 1-sum(diag(myLRtrainConfusion))/sum(myLRtrainConfusion) ## [1] 0.1351351 myLRtrainpred <- predict(myLRtrain, newdata=myBrokerDF) (myLRtrainCM <- table(myBrokerDF$Up_Down, myLRtrainpred)) ## myLRtrainpred ## Down Up ## Down 105 1 ## Up 4 78 1-sum(diag(myLRtrainCM))/sum(myLRtrainCM) ## [1] 0.02659574 # ROC myLRPredict <- predict(myLRtrain, newdata=myBrokerDF, type=\"prob\") myLRPred <- prediction(myLRPredict[,2], myBrokerDF$Up_Down, label.ordering=c( \"Down\", \"Up\")) myLRPerf <- performance(myLRPred, \"tpr\", \"fpr\") performance(myLRPred, \"auc\") ## An object of class \"performance\" ## Slot \"x.name\": ## [1] \"None\" ## ## Slot \"y.name\": ## [1] \"Area under the ROC curve\" ## ## Slot \"alpha.name\": ## [1] \"none\" ## ## Slot \"x.values\": ## list() ## ## Slot \"y.values\": ## [[1]] ## [1] 0.9983893 ## ## ## Slot \"alpha.values\": ## list() The misclassification rate for the logistic regression model when validating the test set (myLRtraintest) is 13.51%. The accuracy rate for the model was 86.49%. The misclassification rate for the logistic regression model when validating the entire data set (myLRtrainpred) is 2.65%. The accuracy rate for the model was 97.35%. plot(myLRPerf, col=1) plot(myRparttunePerf, col=2, add=TRUE) legend(0.7, 0.6, c(\"Log. Reg.\", \"Class. Tree\"), col=1:2, lwd=10)","title":"Logistic Regression"},{"location":"Prediction/#results-gwp-2019_1","text":"The performance for the logistic regression model and the classification tree model are represented by the red (Class Tree) and black (Log Reg) curves. The AUC for the Class Tree is 87.51 and the AUC for the Log Reg is 99.11.","title":"Results: GWP 2019"},{"location":"Prediction/#conclusion","text":"To conclude with regards to the Broker Segmentation, task 1: the hierarchical and k-means clustering methods were performed and analyzed. Of the two methods, we preferred the k-means clustering approach. Although both methods performed well during evaluation, the K-means clustering approach appeared to perform slightly better and provided better results. The PCA scores and average silhouette coefficients obtained from this method presented better assignment of clusters overall. With regards to Gross Written Premium Prediction, task 2: In comparison to the classification tree and logistic regression, the random forest method, appeared to perform better in the ROC curves. The accuracy rate was 94.69% and misclassification rate 5.32%. This method also provided the highest area under the curve (AUC) = 99%, and high rates for specificity (95%) and sensitivity (94%). While this method seems to be the highest performer, we considered that the results were unrealistic or \u201ctoo good to be true\u201d and high performance could be attributed to data leakage or noise. At times when a complex model, like random forest is used, in a small data set such as the one utilized in this case, the function does not have enough information to train on. Hence, the the best approach for GWP 2018 prediction is the Logistic regression model. The predicted outcomes appeared more meaningful and useful for consideration, and the model seemed to provide more realistic results. Performance results obtained from the model were satisfactory with low misclassification rate of 19.68% and elevated accuracy rate of 80.32%. The AUC was 91.12%, sensitivity 78% and specificity 82%. The features most important for prediction were average quote counts for 2015, average total insured value for 2015 and 2017, hit ratio for 2015. To predict whether the gross premium will increase or decrease for 2019: the logistic regression model appeared to perform better in the ROC curve. The accuracy rate for this model was 97.35% and the misclassification rate was 2.65%. The AUC for this model was 99%, specificity was 98%. and sensitivity was 96.34 %. Again, due to unrealistic performance results, we would prefer the other method. Our recommendation for 2019 prediction has to be the classification tree model as it also provided high performance values. The AUC for this model was 87.51%, specificity was 84% and sensitivity was 85.3%. The misclassification rate obtained was 15.43% and the accuracy rate for this model was 84.57%. The most important features for prediction appear to be GWP 2016, 2017, and 2018, policy counts 2016, 2017, and 2018, as well as the avg. quote for 2017 and 2018, and avg. TIV for 2017 and 2018.","title":"Conclusion"}]}